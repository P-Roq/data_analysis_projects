{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0796b9a",
   "metadata": {},
   "source": [
    "# Spam Filter - Alternative Algorithm Versions (II, III and IV)\n",
    "\n",
    "\n",
    "**This notebook compiles the core structure of the algorithm built in Project 14.**\n",
    "\n",
    "Algorithm's purpose(s):\n",
    "\n",
    "- 1st, determine if a SMS (cellphone message) is spam or not.\n",
    "- 2nd, return a percentage of accuracy when applying it to a test data set comprised of SMSs (pre-evaluated as spam or not spam).\n",
    "\n",
    "In each version/script we tweak two sections of the algorithm where the input messages (strings) are divided into a list of words: \n",
    "\n",
    "- Section 1 - in the training section of the algorithm.\n",
    "- Section 2 - inside the `classify_test_set` function.\n",
    "    \n",
    "Notes: \n",
    "\n",
    "- these section's boundaries are marked as comment with the names 'Tweak section 1' and 'Tweak section 2'.\n",
    "\n",
    "- the case sensitive versions are omitted for sake of readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "drawn-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb06d56",
   "metadata": {},
   "source": [
    "## Version II - Recognize currency symbols: £, € and \\$\n",
    "---\n",
    "\n",
    "In this version we simply make the list of words derived from the message recognize currency symbols.\n",
    "\n",
    "Full script version II:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab2c3a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When applied to the messages in the `training_set` (1114 entries) the test accuracy was aprox. 98.83%.\n"
     ]
    }
   ],
   "source": [
    "sms_spam_full = pd.read_csv(\n",
    "    'SMSSpamCollection.txt',\n",
    "    sep='\\t',\n",
    "    names=['Label', 'SMS']\n",
    ")\n",
    "\n",
    "random_sms_spam = sms_spam_full.sample(n=None, frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "training_set = random_sms_spam.copy().iloc[:4458+1, :]\n",
    "\n",
    "testing_set = random_sms_spam.copy().iloc[4458:, :]\n",
    "\n",
    "# Training set.\n",
    "count_label_training = training_set.Label.value_counts(normalize=True).round(3)*100\n",
    "\n",
    "count_label_training = count_label_training.rename('ham vs spam (%)')\n",
    "\n",
    "# Testing set.\n",
    "count_label_testing = testing_set.Label.value_counts(normalize=True).round(3)*100\n",
    "\n",
    "count_label_testing = count_label_testing.rename('ham vs spam (%)')\n",
    "\n",
    "#---------------------------------------\n",
    "\n",
    "# Free RAM 1.\n",
    "del sms_spam_full\n",
    "del random_sms_spam\n",
    "\n",
    "\n",
    "### Tweak section 1 (start) - how a message is splitted into a list of words.\n",
    "\n",
    "# Training set `SMS` cleaned series\n",
    "\n",
    "# This replacement removes everything that is not a letter, number,\n",
    "# whitespace, or one of the three currency symbols: £, € and $. \n",
    "cleaned = training_set.SMS.copy().str.replace('[^A-Za-z0-9\\s£€\\$]', ' ', regex=True)\n",
    "# This replaccement finds the currency symbols and inserts whitespaces at the left and right, as boundaries.\n",
    "cleaned = cleaned.str.replace('(£|€|\\$)', r' \\1 ', regex=True)\n",
    "       \n",
    "#-------------------------------------------------------------        \n",
    "        \n",
    "#`\\s+` ensures that if there are two or more joined whitespaces they are converted to just one.\n",
    "cleaned = cleaned.str.replace('\\s+', ' ', regex=True) \n",
    "\n",
    "cleaned = cleaned.str.replace('(\\A +| +\\Z)', '', regex=True)\n",
    "\n",
    "# Lower case for every string.\n",
    "cleaned = cleaned.str.lower()\n",
    "\n",
    "### Tweak section 1 (end).\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "cleaned_split = cleaned.str.split(' ', expand=True) \n",
    "\n",
    "cleaned_split_cat = pd.concat([cleaned_split.iloc[i, :] for i in range(0, cleaned_split.shape[0])], ignore_index=True)\n",
    "\n",
    "cleaned_split_cat = cleaned_split_cat.dropna()\n",
    "\n",
    "cleaned_split_cat_to_list = cleaned_split_cat.to_list()\n",
    "\n",
    "vocabulary_set = set(cleaned_split_cat_to_list)\n",
    "\n",
    "vocabulary = list(vocabulary_set)\n",
    "\n",
    "for index, el in enumerate(vocabulary):\n",
    "    if el == '':\n",
    "        del vocabulary[index]\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "        \n",
    "# Free RAM 2.\n",
    "del cleaned_split_cat \n",
    "del cleaned_split_cat_to_list\n",
    "del vocabulary_set\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "def remove_elements(list_x, list_strings):\n",
    "    \"\"\"Strings in list_strings are removed from list_x if this later \n",
    "    list contains any of those strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    for index, el in enumerate(list_x):\n",
    "        if el in list_strings:\n",
    "            del list_x[index]\n",
    "    \n",
    "    return list_x\n",
    "\n",
    "# `expand=False` by default.\n",
    "cleaned_split_listed = cleaned.str.split(' ') \n",
    "\n",
    "strings_to_remove = ['']\n",
    "\n",
    "cleaned_split_listed_1 = cleaned_split_listed.copy().apply(lambda x: remove_elements(x, strings_to_remove))\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "word_counts_per_sms = {unique_word: [0] * len(cleaned_split_listed_1) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(cleaned_split_listed_1):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1\n",
    "                   \n",
    "# Convert dictionary into DataFrame.\n",
    "word_counts_per_sms_df = pd.DataFrame(word_counts_per_sms)\n",
    "\n",
    "# `sort=False` is required to preserve the order of the columns in a 'first in' fashion.\n",
    "training_set_2 = pd.concat([training_set, word_counts_per_sms_df], axis=1, sort=False)\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "# Free RAM 3.\n",
    "del cleaned_split_listed\n",
    "del word_counts_per_sms\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "label_counts_ts2 = training_set_2.Label.value_counts(normalize=True)\n",
    "\n",
    "p_spam = label_counts_ts2.spam\n",
    "\n",
    "p_ham = label_counts_ts2.ham\n",
    "\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "training_set_2['sum_words_sms'] = training_set_2.iloc[:, 2:].copy().sum(axis=1)\n",
    "\n",
    "n_spam = training_set_2.loc[training_set_2.Label=='spam', 'sum_words_sms'].sum()\n",
    "\n",
    "n_ham = training_set_2.loc[training_set_2.Label=='ham', 'sum_words_sms'].sum()\n",
    "\n",
    "alpha = 1 \n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "sms_spam = training_set_2.iloc[:, 2:-1].copy()[training_set_2.Label=='spam']\n",
    "\n",
    "sms_ham = training_set_2.iloc[:, 2:-1].copy()[training_set_2.Label=='ham']\n",
    "\n",
    "sms_spam_sum = sms_spam.sum().transpose()\n",
    "\n",
    "sms_ham_sum = sms_ham.sum().transpose()\n",
    "\n",
    "p_wi_given_spam_dict = {}\n",
    "\n",
    "p_wi_given_ham_dict = {}\n",
    "\n",
    "# P(w_i|Spam)\n",
    "for i in range(0, sms_spam_sum.size):\n",
    "    index = sms_spam_sum.index[i]\n",
    "    dividend = sms_spam_sum[index] + alpha\n",
    "    divisor = n_spam + (alpha*n_vocabulary)\n",
    "    p_wi_given_spam_dict[index] =  dividend / divisor\n",
    "\n",
    "    \n",
    "# P(w_i|Ham)\n",
    "for i in range(0, sms_ham_sum.size):\n",
    "    index = sms_ham_sum.index[i]\n",
    "    dividend = sms_ham_sum[index] + alpha\n",
    "    divisor = n_ham + (alpha*n_vocabulary)\n",
    "    p_wi_given_ham_dict[index] =  dividend / divisor\n",
    "    \n",
    "#-------------------------------------------------------------\n",
    "\n",
    "# Free RAM 4.\n",
    "del sms_spam\n",
    "del sms_ham\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "def classify_test_set(message):\n",
    "    \"\"\"Takes in a string - a cellphone message (SMS), and returns a classification of whether \n",
    "    the message is spam, not spam (ham), or if a human is required to classify the message.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Tweak section 2 (start) - how a message is splitted into a list of words\n",
    "\n",
    "    message = re.sub('[^A-Za-z0-9\\s£€\\$]', ' ', message) # still a string\n",
    "    message = re.sub('(£|€|\\$)', r' \\1 ', message)\n",
    "    message = re.sub('\\s+', ' ', message)\n",
    "    message = message.lower() # still a string\n",
    "    message = message.split() # now a list of strings\n",
    "    \n",
    "    ### Tweak section 2 (end)\n",
    "\n",
    "    # Calculating P(Spam|w_1, w_2, ..., w_n) with P(Ham|w_1, w_2, ..., w_n).\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    # Note: if `word` is not in the spam or in the non-spam DataFrames the loop does nothing by default.\n",
    "    for word in message:\n",
    "        \n",
    "        if word in p_wi_given_spam_dict.keys():\n",
    "            p_spam_given_message *= p_wi_given_spam_dict[word]\n",
    "            \n",
    "        if word in p_wi_given_ham_dict.keys():\n",
    "            p_ham_given_message *= p_wi_given_ham_dict[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'Requires human classification.'\n",
    "    \n",
    "\n",
    "testing_set['Test'] = testing_set['SMS'].apply(classify_test_set)\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "# Assigns True if condition is met and multplying by one converts True in '1' and False in '0'.\n",
    "testing_set['Correct'] = (testing_set['Label'] == testing_set['Test'])*1\n",
    "\n",
    "test_accuracy = (testing_set['Correct'].sum() / testing_set.shape[0])*100\n",
    "\n",
    "test_accuracy = test_accuracy.round(2)\n",
    "\n",
    "print(f'When applied to the messages in the `training_set` ({testing_set.shape[0]} entries) the test accuracy was aprox. {test_accuracy}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3d08c",
   "metadata": {},
   "source": [
    "## Version III - Recognize currency symbols: £, € and \\$, and references to GBP pences\n",
    "---\n",
    "\n",
    "We've seen in the previous version that this innovation will marginally improve the algorithm's accuracy, but some money references were still not accounted for.\n",
    "\n",
    "We can tell that we are working with a set of messages presumably from UK, since the money/currency references made are either 'GBP' or pences. When filtering the training set for 'pence' or other proxy we realize that there are many variations of this 'tell tell' sign of spamming. Some variations on 'pence' include '150pm', which stands for '150 per message' can be seen below. \n",
    "\n",
    "The regex expression `pattern_pences`, looks for every sequence of characters that might include a sequence of characters composed of a single to three digits (`\\d{1, 3}`) followed immediately by the word `p`. This limitation is due to the fact that we know by default that references to pences range usually from 5 to 150 (or more), but not in the thousands range. Expected matches are: \n",
    "\n",
    "|sub-pattern  |  example                 | \n",
    "|---------|--------------------------|\n",
    "|  \\d+p\\w+ |'**150p**pmsg'   | \n",
    "|  \\d+p\\w+ |'**50p**erwksub'   | \n",
    "|  \\w+\\d+p | '08712400602**450p**'  | \n",
    "|  \\w+\\d{1,}p\\w+ | 'com1win**150p**pmx3age16'  | \n",
    "\n",
    "From the table above we can notice that in some situations, such as in the third example (from the top), some words may not be referencing pences, other cases can be ambiguous, such as the second example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "087163f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['08700621170150p',\n",
       " '150pm',\n",
       " '150p',\n",
       " '10p',\n",
       " '150p',\n",
       " '150p',\n",
       " '50pmmorefrommobile2bremoved',\n",
       " 'mobstorequiz10ppm',\n",
       " 'callcost150ppmmobilesvary',\n",
       " '450pw']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_patterns = []\n",
    "\n",
    "pattern_pences = '\\\\b\\w*\\d{1,3}p\\w*\\\\b'\n",
    "\n",
    "for i, val in enumerate(cleaned):\n",
    "    pences = re.findall(pattern_pences, val)\n",
    "    if len(pences):\n",
    "        list_of_patterns.append(pences[0])\n",
    "        \n",
    "list_of_patterns[:10]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a538e",
   "metadata": {},
   "source": [
    "One attempt to make the algorithm recognize references to pences is to separate expressions like the ones previously mentioned, e.g. '10p', '150p', from the rest of the characters with a whitespace. The way to achieve this is to:\n",
    "\n",
    "- first, identify a list of patterns (which is already done above).\n",
    "- second, create a modified string compiled in a `list_of_replacements`.\n",
    "- third and last, replacing in the main data set the old expression for the new expression, inside the same message, resorting to the `replace` function.\n",
    "\n",
    "\n",
    "Example of a message modification: `training_set.SMS[186]`, contains this expression: 'callcost150ppmmobilesvary'.\n",
    "\n",
    "In its former form:\n",
    "\n",
    "- 'URGENT  This is the 2nd attempt to contact U U have WON  £ 1000CALL 09071512432 b4 300603t csBCM4235WC1N3XX callcost150ppmmobilesvary  max £ 7  50'\n",
    "\n",
    "In its later form, after the modification:\n",
    "\n",
    "- 'URGENT  This is the 2nd attempt to contact U U have WON  £ 1000CALL 09071512432 b4 300603t csBCM4235WC1N3XX callcost 150p pmmobilesvary  max £ 7  50'\n",
    "\n",
    "\n",
    "The list of replacements is built as follows:\n",
    "\n",
    "Note: in the first loop the condition `re.findall('([1-9]pm|1[0-2]pm)', val_1) == []` was set so that the evening time references are not recognize, namely 1pm to 12pm. Despite of clearing some mis-interpreted pences references, a small number will not be recognized though, e.g. if there is a message that has the expression '10pm', meaning '10 pences per message' instead of '10 p.m. in the evening', it will not be considered as a reference to pences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd60c3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['08700621170 150p ',\n",
       " ' 150p m',\n",
       " ' 150p ',\n",
       " ' 10p ',\n",
       " ' 150p ',\n",
       " ' 150p ',\n",
       " ' 50p mmorefrommobile2bremoved',\n",
       " 'mobstorequiz 10p pm',\n",
       " 'callcost 150p pmmobilesvary',\n",
       " ' 450p w']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_replacements = []\n",
    "pences_references = []\n",
    "\n",
    "for i, val_1 in enumerate(list_of_patterns):\n",
    "    if re.findall('([1-9]pm|1[0-2]pm)', val_1) == []:\n",
    "        pence_ref = val_1\n",
    "        digits_p = re.findall('\\d{1,3}p', pence_ref)\n",
    "        pences_references.append(digits_p)\n",
    "\n",
    "    final_str = ''\n",
    "    \n",
    "    for val_2 in digits_p:\n",
    "        \n",
    "        sub = ' ' + val_2 + ' '\n",
    "        \n",
    "        if final_str == '':\n",
    "            final_str = re.sub(val_2, sub, pence_ref, count=1)\n",
    "            \n",
    "        else:\n",
    "            final_str = re.sub(val_2, sub, final_str, count=1)\n",
    "    \n",
    "    list_of_replacements.append(final_str)\n",
    "\n",
    "list_of_replacements[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f893f",
   "metadata": {},
   "source": [
    "The following function, when applied to a string, looks for expressions that may be in the `list_of_patterns` and if there is a match, it replaces that expression for the equivalent version (with whitespaces separating letters from words, as seen previously), resorting to the `list_of_replacements`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eec000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(string):\n",
    "    \n",
    "    new_string = ''\n",
    "    \n",
    "    for i, val in enumerate(list_of_patterns):\n",
    "        if val in string:\n",
    "            new_string += re.sub(val, list_of_replacements[i], string)\n",
    "            break\n",
    "            \n",
    "    if len(new_string):\n",
    "        return new_string\n",
    "    else:\n",
    "        return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d93199",
   "metadata": {},
   "source": [
    "Verifying changes made by the function in two random spam messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3143529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urgent this is the 2nd attempt to contact u u have won £ 1000call 09071512432 b4 300603t csbcm4235wc1n3xx callcost150ppmmobilesvary max £ 7 50\n",
      "\n",
      "urgent this is the 2nd attempt to contact u u have won £ 1000call 09071512432 b4 300603t csbcm4235wc1n3xx callcost 150p pmmobilesvary max £ 7 50\n"
     ]
    }
   ],
   "source": [
    "# the one from the previous example\n",
    "print(cleaned[186], replace(cleaned[186]), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79de89ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "someone has contacted our dating service and entered your phone because they fancy you to find out who it is call from a landline 09111032124 pobox12n146tf150p\n",
      "\n",
      "someone has contacted our dating service and entered your phone because they fancy you to find out who it is call from a landline 09111032124 pobox12n146tf 150p \n"
     ]
    }
   ],
   "source": [
    "print(cleaned[2821], replace(cleaned[2821]), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b704165",
   "metadata": {},
   "source": [
    "The full version III script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4f82b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When applied to the messages in the `training_set` (1114 entries) the test accuracy was aprox. 98.83%.\n"
     ]
    }
   ],
   "source": [
    "sms_spam_full = pd.read_csv(\n",
    "    'SMSSpamCollection.txt',\n",
    "    sep='\\t',\n",
    "    names=['Label', 'SMS']\n",
    ")\n",
    "\n",
    "random_sms_spam = sms_spam_full.sample(n=None, frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "training_set = random_sms_spam.copy().iloc[:4458+1, :]\n",
    "\n",
    "testing_set = random_sms_spam.copy().iloc[4458:, :]\n",
    "\n",
    "# Training set.\n",
    "count_label_training = training_set.Label.value_counts(normalize=True).round(3)*100\n",
    "\n",
    "count_label_training = count_label_training.rename('ham vs spam (%)')\n",
    "\n",
    "# Testing set.\n",
    "count_label_testing = testing_set.Label.value_counts(normalize=True).round(3)*100\n",
    "\n",
    "count_label_testing = count_label_testing.rename('ham vs spam (%)')\n",
    "\n",
    "#---------------------------------------\n",
    "\n",
    "# Saving RAM 1.\n",
    "del sms_spam_full\n",
    "del random_sms_spam\n",
    "\n",
    "### Tweak section 1 (start) - how a message is splitted into a list of words.\n",
    "\n",
    "# Training set `SMS` cleaned series\n",
    "cleaned = training_set.SMS.copy().str.replace('[^A-Za-z0-9\\s£€\\$]', ' ', regex=True)\n",
    "cleaned = cleaned.str.replace('€', ' € ', regex=False)\n",
    "cleaned = cleaned.str.replace('£', ' £ ', regex=True)\n",
    "cleaned = cleaned.str.replace('\\$', ' $ ', regex=True)\n",
    "\n",
    "# 1st, identify a list of patterns (which is already done above).\n",
    "list_of_patterns = []\n",
    "\n",
    "pattern_pences = '\\\\b\\w*\\d{1,3}p\\w*\\\\b'\n",
    "\n",
    "for i, val in enumerate(cleaned):\n",
    "    pences = re.findall(pattern_pences, val)\n",
    "    if len(pences):\n",
    "        list_of_patterns.append(pences[0])\n",
    "        \n",
    "# 2nd, create a modified string compiled in a `list_of_replacements`.        \n",
    "list_of_replacements = []\n",
    "pences_references = []\n",
    "\n",
    "for i, val_1 in enumerate(list_of_patterns):\n",
    "    if re.findall('([1-9]pm|1[0-2]pm)', val_1) == []:\n",
    "        pence_ref = val_1\n",
    "        digits_p = re.findall('\\d{1,3}p', pence_ref)\n",
    "        pences_references.append(digits_p)\n",
    "\n",
    "    final_str = ''\n",
    "    \n",
    "    for val_2 in digits_p:\n",
    "        \n",
    "        sub = ' ' + val_2 + ' '\n",
    "        \n",
    "        if final_str == '':\n",
    "            final_str = re.sub(val_2, sub, pence_ref, count=1)\n",
    "            \n",
    "        else:\n",
    "            final_str = re.sub(val_2, sub, final_str, count=1)\n",
    "    \n",
    "    list_of_replacements.append(final_str)\n",
    "        \n",
    "# 3rd, replacing in the main data set the old expression for the new expression inside the same message resorting to the `replace` function.\n",
    "\n",
    "def replace(string):\n",
    "    \n",
    "    new_string = ''\n",
    "    \n",
    "    for i, val in enumerate(list_of_patterns):\n",
    "        if val in string:\n",
    "            new_string += re.sub(val, list_of_replacements[i], string)\n",
    "            break\n",
    "            \n",
    "    if len(new_string):\n",
    "        return new_string\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "\n",
    "cleaned = cleaned.apply(replace)\n",
    "\n",
    "#`\\s+` ensures that if there are two or more joined whitespaces they are converted to just one.\n",
    "cleaned = cleaned.str.replace('\\s+', ' ', regex=True) \n",
    "\n",
    "cleaned = cleaned.str.replace('(\\A +| +\\Z)', '', regex=True)\n",
    "\n",
    "# Lower case for every string.\n",
    "cleaned = cleaned.str.lower()\n",
    "\n",
    "### Tweak section 1 (end).\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "cleaned_split = cleaned.str.split(' ', expand=True) \n",
    "\n",
    "cleaned_split_cat = pd.concat([cleaned_split.iloc[i, :] for i in range(0, cleaned_split.shape[0])], ignore_index=True)\n",
    "\n",
    "cleaned_split_cat = cleaned_split_cat.dropna()\n",
    "\n",
    "cleaned_split_cat_to_list = cleaned_split_cat.to_list()\n",
    "\n",
    "vocabulary_set = set(cleaned_split_cat_to_list)\n",
    "\n",
    "vocabulary = list(vocabulary_set)\n",
    "\n",
    "for index, el in enumerate(vocabulary):\n",
    "    if el == '':\n",
    "        del vocabulary[index]\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "        \n",
    "# Free RAM 2.\n",
    "del cleaned_split_cat \n",
    "del cleaned_split_cat_to_list\n",
    "del vocabulary_set\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "def remove_elements(list_x, list_strings):\n",
    "    \"\"\"Strings in list_strings are removed from list_x if this later \n",
    "    list contains any of those strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    for index, el in enumerate(list_x):\n",
    "        if el in list_strings:\n",
    "            del list_x[index]\n",
    "    \n",
    "    return list_x\n",
    "\n",
    "\n",
    "# `expand=False` by default.\n",
    "cleaned_split_listed = cleaned.str.split(' ') \n",
    "\n",
    "\n",
    "strings_to_remove = ['']\n",
    "\n",
    "cleaned_split_listed_1 = cleaned_split_listed.copy().apply(lambda x: remove_elements(x, strings_to_remove))\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "word_counts_per_sms = {unique_word: [0] * len(cleaned_split_listed_1) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(cleaned_split_listed_1):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1\n",
    "        \n",
    "              \n",
    "# Convert dictionary into DataFrame.\n",
    "word_counts_per_sms_df = pd.DataFrame(word_counts_per_sms)\n",
    "\n",
    "\n",
    "# `sort=False` is required to preserve the order of the columns in a 'first in' fashion.\n",
    "training_set_2 = pd.concat([training_set, word_counts_per_sms_df], axis=1, sort=False)\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "# Free RAM 3.\n",
    "del cleaned_split_listed\n",
    "del word_counts_per_sms\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "label_counts_ts2 = training_set_2.Label.value_counts(normalize=True)\n",
    "\n",
    "p_spam = label_counts_ts2.spam\n",
    "\n",
    "p_ham = label_counts_ts2.ham\n",
    "\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "training_set_2['sum_words_sms'] = training_set_2.iloc[:, 2:].copy().sum(axis=1)\n",
    "\n",
    "n_spam = training_set_2.loc[training_set_2.Label=='spam', 'sum_words_sms'].sum()\n",
    "\n",
    "n_ham = training_set_2.loc[training_set_2.Label=='ham', 'sum_words_sms'].sum()\n",
    "\n",
    "alpha = 1 \n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "# The last colum 'sum_words_sms' is not included in the calculations\n",
    "# of these Series, so I set `.iloc[:, 2:-1]`.\n",
    "sms_spam = training_set_2.iloc[:, 2:-1].copy()[training_set_2.Label=='spam']\n",
    "\n",
    "sms_ham = training_set_2.iloc[:, 2:-1].copy()[training_set_2.Label=='ham']\n",
    "\n",
    "sms_spam_sum = sms_spam.sum().transpose()\n",
    "\n",
    "sms_ham_sum = sms_ham.sum().transpose()\n",
    "\n",
    "p_wi_given_spam_dict = {}\n",
    "\n",
    "p_wi_given_ham_dict = {}\n",
    "\n",
    "# P(w_i|Spam)\n",
    "for i in range(0, sms_spam_sum.size):\n",
    "    index = sms_spam_sum.index[i]\n",
    "    dividend = sms_spam_sum[index] + alpha\n",
    "    divisor = n_spam + (alpha*n_vocabulary)\n",
    "    p_wi_given_spam_dict[index] =  dividend / divisor\n",
    "\n",
    "    \n",
    "# P(w_i|Ham)\n",
    "for i in range(0, sms_ham_sum.size):\n",
    "    index = sms_ham_sum.index[i]\n",
    "    dividend = sms_ham_sum[index] + alpha\n",
    "    divisor = n_ham + (alpha*n_vocabulary)\n",
    "    p_wi_given_ham_dict[index] =  dividend / divisor\n",
    "    \n",
    "#-------------------------------------------------------------\n",
    "\n",
    "# Free RAM 4.\n",
    "del sms_spam\n",
    "del sms_ham\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "def classify_test_set(message):\n",
    "    \"\"\"Takes in a string - a cellphone message (SMS), and returns a classification of whether \n",
    "    the message is spam, not spam (ham), or if a human is required to classify the message.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Tweak section 2 (start) - how a message is splitted into a list of words\n",
    "\n",
    "    message = re.sub('[^A-Za-z0-9\\s£€\\$]', ' ', message) # still a string\n",
    "    message = re.sub('£', ' £ ', message)\n",
    "    message = re.sub('€', ' € ', message)\n",
    "    message = re.sub('\\$', ' $ ', message)\n",
    "    message = replace(message)\n",
    "    message = re.sub('\\s+', ' ', message)\n",
    "    message = message.lower() # still a string\n",
    "    message = message.split() # now a list of strings\n",
    "    \n",
    "    \n",
    "    ### Tweak section 2 (end).\n",
    "\n",
    "    # Calculating P(Spam|w_1, w_2, ..., w_n) with P(Ham|w_1, w_2, ..., w_n).\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    # Note: if `word` is not in the spam or in the non-spam DataFrames the loop does nothing by default.\n",
    "    for word in message:\n",
    "        \n",
    "        if word in p_wi_given_spam_dict.keys():\n",
    "            p_spam_given_message *= p_wi_given_spam_dict[word]\n",
    "            \n",
    "        if word in p_wi_given_ham_dict.keys():\n",
    "            p_ham_given_message *= p_wi_given_ham_dict[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'Requires human classification.'\n",
    "    \n",
    "\n",
    "testing_set['Test'] = testing_set['SMS'].apply(classify_test_set)\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "# Assigns True if condition is met and multplying by one converts True in '1' and False in '0'.\n",
    "testing_set['Correct'] = (testing_set['Label'] == testing_set['Test'])*1\n",
    "\n",
    "test_accuracy = (testing_set['Correct'].sum() / testing_set.shape[0])*100\n",
    "\n",
    "test_accuracy = test_accuracy.round(2)\n",
    "\n",
    "print(f'When applied to the messages in the `training_set` ({testing_set.shape[0]} entries) the test accuracy was aprox. {test_accuracy}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b082c",
   "metadata": {},
   "source": [
    "## Version IV - Separate all numbers from letters and from other characters/symbols.\n",
    "---\n",
    "\n",
    "For this version, in section 1 and 2, instead of having all 'non-words' removed ('\\w'), we identify digits and symbols and insert white spaces as boundaries:\n",
    "\n",
    "    cleaned = training_set.\\\n",
    "        SMS.copy().\\\n",
    "        str.replace('(\\W|\\d+)', r' \\1 ', regex=True)\n",
    "\n",
    "    message = re.sub('(\\W|\\d+)', r' \\1 ', message)\n",
    "    \n",
    "The sub parameter, `r' \\1 '`, identifies any pattern given by `'(\\W|\\d+)'` in a string and inserts the whitespaces left and right.\n",
    "\n",
    "An example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90e90634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URGENT! This is the 2nd attempt to contact U!U have WON £1000CALL 09071512432 b4 300603t&csBCM4235WC1N3XX.callcost150ppmmobilesvary. max£7. 50\n",
      "\n",
      "URGENT !    This   is   the    2 nd   attempt   to   contact   U ! U   have   WON    £  1000 CALL    09071512432    b 4     300603 t & csBCM 4235 WC 1 N 3 XX . callcost 150 ppmmobilesvary .    max £  7  .     50 \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    training_set.SMS[186], \n",
    "    re.sub('(\\W|\\d+)', r' \\1 ', training_set.SMS[186]),\n",
    "    sep='\\n\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80370a56",
   "metadata": {},
   "source": [
    "The full version IV script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5b66414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When applied to the messages in the `testing_set` (1114 entries) the test accuracy was aprox. 98.65%.\n"
     ]
    }
   ],
   "source": [
    "sms_spam_full = pd.read_csv(\n",
    "    'SMSSpamCollection.txt',\n",
    "    sep='\\t',\n",
    "    names=['Label', 'SMS']\n",
    ")\n",
    "\n",
    "random_sms_spam = sms_spam_full.sample(n=None, frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "training_set = random_sms_spam.copy().iloc[:4458+1, :]\n",
    "\n",
    "testing_set = random_sms_spam.copy().iloc[4458:, :]\n",
    "\n",
    "# Training set.\n",
    "count_label_training = training_set.Label.value_counts(normalize=True).round(3)*100\n",
    "\n",
    "count_label_training = count_label_training.rename('ham vs spam (%)')\n",
    "\n",
    "# Testing set.\n",
    "count_label_testing = testing_set.Label.value_counts(normalize=True).round(3)*100\n",
    "\n",
    "count_label_testing = count_label_testing.rename('ham vs spam (%)')\n",
    "\n",
    "#---------------------------------------\n",
    "\n",
    "# Free RAM 1.\n",
    "del sms_spam_full\n",
    "del random_sms_spam\n",
    "\n",
    "### Tweak section 1 (start) - how a message is splitted into a list of words,\n",
    "\n",
    "cleaned = training_set.SMS.copy().str.replace('(\\W|\\d+)', r' \\1 ', regex=True)\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "#`\\s+` ensures that if there are two or more joined whitespaces they are converted to just one.\n",
    "cleaned = cleaned.str.replace('\\s+', ' ', regex=True) \n",
    "\n",
    "cleaned = cleaned.str.replace('(\\A +| +\\Z)', '', regex=True) \n",
    "\n",
    "# Lower case for every string.\n",
    "cleaned = cleaned.str.lower()\n",
    "\n",
    "### Tweak section 1 (end).\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "cleaned_split = cleaned.str.split(' ', expand=True) \n",
    "\n",
    "cleaned_split_cat = pd.concat([cleaned_split.iloc[i, :] for i in range(0, cleaned_split.shape[0])], ignore_index=True)\n",
    "\n",
    "cleaned_split_cat = cleaned_split_cat.dropna()\n",
    "\n",
    "cleaned_split_cat_to_list = cleaned_split_cat.to_list()\n",
    "\n",
    "vocabulary_set = set(cleaned_split_cat_to_list)\n",
    "\n",
    "vocabulary = list(vocabulary_set)\n",
    "\n",
    "for index, el in enumerate(vocabulary):\n",
    "    if el == '':\n",
    "        del vocabulary[index]\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "        \n",
    "# Free RAM 2.\n",
    "del cleaned_split_cat \n",
    "del cleaned_split_cat_to_list\n",
    "del vocabulary_set\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "def remove_elements(list_x, list_strings):\n",
    "    \"\"\"Strings in list_strings are removed from list_x if this later \n",
    "    list contains any of those strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    for index, el in enumerate(list_x):\n",
    "        if el in list_strings:\n",
    "            del list_x[index]\n",
    "    \n",
    "    return list_x\n",
    "\n",
    "# `expand=False` by default.\n",
    "cleaned_split_listed = cleaned.str.split(' ') \n",
    "\n",
    "strings_to_remove = ['']\n",
    "\n",
    "cleaned_split_listed_1 = cleaned_split_listed.copy().apply(lambda x: remove_elements(x, strings_to_remove))\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "word_counts_per_sms = {unique_word: [0] * len(cleaned_split_listed_1) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(cleaned_split_listed_1):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1\n",
    "        \n",
    "              \n",
    "# Convert dictionary into DataFrame\n",
    "word_counts_per_sms_df = pd.DataFrame(word_counts_per_sms)\n",
    "\n",
    "\n",
    "# `sort=False` is required to preserve the order of the columns in a 'first in' fashion.\n",
    "training_set_2 = pd.concat([training_set, word_counts_per_sms_df], axis=1, sort=False)\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "# Free RAM 3.\n",
    "del cleaned_split_listed\n",
    "del word_counts_per_sms\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "label_counts_ts2 = training_set_2.Label.value_counts(normalize=True)\n",
    "\n",
    "p_spam = label_counts_ts2.spam\n",
    "\n",
    "p_ham = label_counts_ts2.ham\n",
    "\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "training_set_2['sum_words_sms'] = training_set_2.iloc[:, 2:].copy().sum(axis=1)\n",
    "\n",
    "n_spam = training_set_2.loc[training_set_2.Label=='spam', 'sum_words_sms'].sum()\n",
    "\n",
    "n_ham = training_set_2.loc[training_set_2.Label=='ham', 'sum_words_sms'].sum()\n",
    "\n",
    "alpha = 1 \n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "# The last colum 'sum_words_sms' is not included in the calculations\n",
    "# of these Series, so I set `.iloc[:, 2:-1]`.\n",
    "sms_spam = training_set_2.iloc[:, 2:-1].copy()[training_set_2.Label=='spam']\n",
    "\n",
    "sms_ham = training_set_2.iloc[:, 2:-1].copy()[training_set_2.Label=='ham']\n",
    "\n",
    "sms_spam_sum = sms_spam.sum().transpose()\n",
    "\n",
    "sms_ham_sum = sms_ham.sum().transpose()\n",
    "\n",
    "p_wi_given_spam_dict = {}\n",
    "\n",
    "p_wi_given_ham_dict = {}\n",
    "\n",
    "# P(w_i|Spam)\n",
    "for i in range(0, sms_spam_sum.size):\n",
    "    index = sms_spam_sum.index[i]\n",
    "    dividend = sms_spam_sum[index] + alpha\n",
    "    divisor = n_spam + (alpha*n_vocabulary)\n",
    "    p_wi_given_spam_dict[index] =  dividend / divisor\n",
    "\n",
    "    \n",
    "# P(w_i|Ham)\n",
    "for i in range(0, sms_ham_sum.size):\n",
    "    index = sms_ham_sum.index[i]\n",
    "    dividend = sms_ham_sum[index] + alpha\n",
    "    divisor = n_ham + (alpha*n_vocabulary)\n",
    "    p_wi_given_ham_dict[index] =  dividend / divisor\n",
    "    \n",
    "#-------------------------------------------------------------\n",
    "\n",
    "# Free RAM 4.\n",
    "del sms_spam\n",
    "del sms_ham\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "\n",
    "def classify_test_set(message):\n",
    "    \"\"\"Takes in a string - a cellphone message (SMS), and returns a classification of whether \n",
    "    the message is spam, not spam (ham), or if a human is required to classify the message.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Tweak section 3 (start) - how a message is splitted into a list of words\n",
    "\n",
    "    message = re.sub('(\\W|\\d+)', r' \\1 ', message)\n",
    "    message = re.sub('\\s+', ' ', message)\n",
    "    message = message.lower() \n",
    "    message = message.split() # now a list of strings\n",
    "    \n",
    "    ### Tweak section 1 (end)\n",
    "\n",
    "    # Calculating P(Spam|w_1, w_2, ..., w_n) with P(Ham|w_1, w_2, ..., w_n).\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    # Note: if `word` is not in the spam or in the non-spam DataFrames the loop does nothing by default.\n",
    "    for word in message:\n",
    "        \n",
    "        if word in p_wi_given_spam_dict.keys():\n",
    "            p_spam_given_message *= p_wi_given_spam_dict[word]\n",
    "            \n",
    "        if word in p_wi_given_ham_dict.keys():\n",
    "            p_ham_given_message *= p_wi_given_ham_dict[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'Requires human classification.'\n",
    "    \n",
    "\n",
    "testing_set['Test'] = testing_set['SMS'].apply(classify_test_set)\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "# Assigns True if condition is met and multplying by one converts True in '1' and False in '0'.\n",
    "testing_set['Correct'] = (testing_set['Label'] == testing_set['Test'])*1\n",
    "\n",
    "test_accuracy = (testing_set['Correct'].sum() / testing_set.shape[0])*100\n",
    "\n",
    "test_accuracy = test_accuracy.round(2)\n",
    "\n",
    "print(f'When applied to the messages in the `testing_set` ({testing_set.shape[0]} entries) the test accuracy was aprox. {test_accuracy}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb780fb",
   "metadata": {},
   "source": [
    "\\***"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
