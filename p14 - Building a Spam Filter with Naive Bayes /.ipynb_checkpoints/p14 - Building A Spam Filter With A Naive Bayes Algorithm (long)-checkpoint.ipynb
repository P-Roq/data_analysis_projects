{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Project 12: Building A Spam Filter With A Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 1\n",
    "---\n",
    "We've come a long way in this course — we've learned to:\n",
    "\n",
    "\n",
    "- Assign probabilities to events based on certain conditions by using conditional probability rules.\n",
    "\n",
    "\n",
    "- Assign probabilities to events based on whether they are in relationship of statistical independence or not with other events.\n",
    "\n",
    "\n",
    "- Assign probabilities to events based on prior knowledge by using Bayes' theorem.\n",
    "\n",
    "\n",
    "- Create a spam filter for SMS messages using the multinomial Naive Bayes algorithm.\n",
    "\n",
    "In our last lesson, we focused extensively on learning how the Naive Bayes algorithm works from a theoretical standpoint (more specifically, we learned about the multinomial Naive Bayes algorithm). In this guided project, we're going to study the practical side of the algorithm by building a spam filter for SMS messages.\n",
    "\n",
    "To classify messages as spam or non-spam, we saw in the previous lesson that the computer:\n",
    "\n",
    "\n",
    "1. Learns how humans classify messages.\n",
    "\n",
    "\n",
    "2. Uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam.\n",
    "\n",
    "\n",
    "3. Classifies a new message based on these probability values — if the probability for spam is greater, then it classifies the message as spam. Otherwise, it classifies it as non-spam (if the two probability values are equal, then we may need a human to classify the message).\n",
    "\n",
    "So our first task is to \"teach\" the computer how to classify messages. To do that, we'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans.\n",
    "\n",
    "The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the [The UCI Machine Learning](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection) Repository.\n",
    "\n",
    "Let's start by reading in the dataset. You'll be able to find the [solutions to this project at this link](https://github.com/dataquestio/solutions/blob/master/Mission433Solutions.ipynb) or by clicking the key icon at the top right of the interface.\n",
    "\n",
    "Note that due to the nature of spam messages, the dataset contains content that may be offensive to some users.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. To help readers gain context into your project, use the first Markdown cell of the notebook to add a title and a short introduction where you concisely explain what the project is about and what your goal is in this project (the title and the introduction are tentative at this point, so don't spend too much time here — you can come back at the end of your work to refine them).\n",
    "\n",
    "\n",
    "2. Open the `SMSSpamCollection` file using the `read_csv()` function from the pandas package.\n",
    "    - The data points are tab separated, so we'll need to use the `sep='\\t'` parameter for our `read_csv()` function.\n",
    "    - The dataset doesn't have a header row, which means we need to use the `header=None` parameter, otherwise the first row will be wrongly used as the header row.\n",
    "    - Use the `names=['Label', 'SMS']` parameter to name the columns as `Label` and `SMS`.\n",
    "\n",
    "\n",
    "3. Explore the dataset a little.\n",
    "    - Find how many rows and columns it has.\n",
    "    - Find what percentage of the messages is spam and what percentage is ham (\"ham\" means non-spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project aims at creating a spam filter that resorts to a multinomial Naive Bayes algorithm in order to distinguish spam messages from regular ones. The filter will be tested on a data set comprised of 5572 messages that have been previously determined by humans as spam or not spam.\n",
    "\n",
    "The spam filter is considered to be successful if it can filter out 80% of the spam from a test set.\n",
    "\n",
    "THe working data set has been made available by Tiago A. Almeida and José María Gómez Hidalgo; it is publicly available at the [The UCI Machine Learning](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection) Repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spam data set to be worked with will be named `sms_spam_full`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_spam_full = pd.read_csv('SMSSpamCollection.txt',\n",
    "                   sep='\\t',\n",
    "                   names=['Label', 'SMS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Label   5572 non-null   object\n",
      " 1   SMS     5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 43.6+ KB\n"
     ]
    }
   ],
   "source": [
    "sms_spam_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Label` column is binary, the values are:\n",
    "\n",
    "- `spam`.\n",
    "- `ham` (not spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are depicted the first five messages in the data set, so that one can observe the general writing style (expected to be informal), and how this can be taken into consideration, when formulating the spam filtering tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                           SMS  \n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  \n",
       "3                                                                                                            U dun say so early hor... U c already then say...  \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "sms_spam_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can see that only aprox. 13.4% of the messages are spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.6\n",
       "spam    13.4\n",
       "Name: ham vs spam (%), dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_label = sms_spam_full.Label.value_counts(normalize=True).round(3)*100\n",
    "\n",
    "count_label = count_label.rename('ham vs spam (%)')\n",
    "\n",
    "count_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 2\n",
    "---\n",
    "On the previous screen, we read in the dataset and saw that about 87% of the messages are ham (\"ham\" means non-spam), and the remaining 13% are spam. Now that we've become a bit familiar with the dataset, we can move on to building the spam filter.\n",
    "\n",
    "However, before creating it, it's very helpful to first think of a way of testing how well it works. When creating software (a spam filter is software), a good rule of thumb is that designing the test comes before creating the software. If we write the software first, then it's tempting to come up with a biased test just to make sure the software passes it.\n",
    "\n",
    "\n",
    "Once our spam filter is done, we'll need to test how good it is with classifying new messages. To test the spam filter, we're first going to split our dataset into two categories:\n",
    "\n",
    "\n",
    "- A **training set**, which we'll use to \"train\" the computer how to classify messages.\n",
    "\n",
    "\n",
    "- A **test set**, which we'll use to test how good the spam filter is with classifying new messages.\n",
    "\n",
    "\n",
    "We're going to keep 80% of our dataset for training, and 20% for testing (we want to train the algorithm on as much data as possible, but we also want to have enough test data). The dataset has 5,572 messages, which means that:\n",
    "\n",
    "\n",
    "- The training set will have 4,458 messages (about 80% of the dataset).\n",
    "\n",
    "\n",
    "- The test set will have 1,114 messages (about 20% of the dataset).\n",
    "\n",
    "To better understand the purpose of putting a test set aside, let's begin by observing that all 1,114 messages in our test set are already classified by a human. When the spam filter is ready, we're going to treat these messages as new and have the filter classify them. Once we have the results, we'll be able to compare the algorithm classification with that done by a human, and this way we'll see how good the spam filter really is.\n",
    "\n",
    "**For this project, our goal is to create a spam filter that classifies new messages with an accuracy greater than 80%** — so we expect that more than 80% of the new messages will be classified correctly as spam or ham (non-spam).\n",
    "\n",
    "We'll come back to testing toward the end of this guided project, but for now, let's create a training and a test set. We're going to start by randomizing the entire dataset to ensure that spam and ham messages are spread properly throughout the dataset. \n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. Start by randomizing the entire dataset by using the `DataFrame.sample()` method.\n",
    "    - Use the `frac=1` parameter to randomize the entire dataset.\n",
    "    - Use the `random_state=1` parameter to make sure your results are reproducible.\n",
    "\n",
    "\n",
    "2. Split the randomized dataset into a training and a test set.\n",
    "    - The training set should account for 80% of the dataset, and the remaining 20% of the data should be the test set.\n",
    "    - Reset the index labels for both data sets — the index labels remained unordered after randomization. You can use the `DataFrame.reset_index()` method.\n",
    "\n",
    "\n",
    "3. Find the percentage of spam and ham in both the training and the test set. Are the percentages similar to what we have in the full dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create a training set and a test set out of `sms_spam_full`:\n",
    "\n",
    "1. Randomize the `sms_spam_full`.\n",
    "\n",
    "\n",
    "2. Split randomized data set into a training set (20%) and a testing set (remaining 80%).\n",
    "\n",
    "\n",
    "3. Compare the 'Label' value distribution of the entire randomized DF with the previously made subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "random_sms_spam = sms_spam_full.sample(n=None, frac=1, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From earlier on we know that `random_sms_spam` has 5572 entries (0 to 5571)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of observations/rows in random_sms_spam: 5572 \n",
      "80% of total observations/rows in random_sms_spam: 4458.0\n"
     ]
    }
   ],
   "source": [
    "eighty_perc = random_sms_spam.shape[0] * 0.8\n",
    "\n",
    "print(f'Total number of observations/rows in random_sms_spam: {random_sms_spam.shape[0]}', \n",
    "      f'\\n80% of total observations/rows in random_sms_spam: {round(eighty_perc, 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the information above we set the training set as the rows 0 to 4458, 80% of the total rows in `random_sms_spam`. The remaining rows, will form the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "training_set = random_sms_spam.copy().iloc[:4458+1, :]\n",
    "\n",
    "testing_set = random_sms_spam.copy().iloc[4458:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, comparing the distribution of values in the `Label` column across DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_sms_spam</th>\n",
       "      <th>count_label_training</th>\n",
       "      <th>count_label_testing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>86.6</td>\n",
       "      <td>86.5</td>\n",
       "      <td>86.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>13.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      random_sms_spam  count_label_training  count_label_testing\n",
       "ham              86.6                  86.5                 86.8\n",
       "spam             13.4                  13.5                 13.2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.\n",
    "\n",
    "# Training set.\n",
    "count_label_training = training_set.Label.value_counts(normalize=True).round(3)*100\n",
    "\n",
    "count_label_training = count_label_training.rename('ham vs spam (%)')\n",
    "\n",
    "# Testing set.\n",
    "count_label_testing = testing_set.Label.value_counts(normalize=True).round(3)*100\n",
    "\n",
    "count_label_testing = count_label_testing.rename('ham vs spam (%)')\n",
    "\n",
    "\n",
    "# Combining all label percentage counts for comparison.\n",
    "compare_label = pd.DataFrame({'random_sms_spam': count_label,\n",
    "                              'count_label_training': count_label_training,\n",
    "                              'count_label_testing': count_label_testing})\n",
    "\n",
    "compare_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the table above, the value distribution is very similar across the panel, meaning that we can infer the conclusions produced from the training set to the testing set, since both sets resemble the original series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving RAM 1\n",
    "del sms_spam_full\n",
    "del random_sms_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: use the training set to teach the algorithm to classify new messages\n",
    "---\n",
    "When a new message arrives and is used as input the Naive Bayes algorithm will make the classification based on the results it gets from these two equations (\"$Spam^C$\" and \"$Ham$\" can be used interchangeably in this case):\n",
    "\n",
    "\n",
    "$P(Ham|w_1, w_2, ..., w_n) > P(Spam|w_1, w_2, ..., w_n)$\n",
    "\n",
    "Building a Naive Bayes algorithm entails two further intertwined steps. The final goal is two take a random new message as input, compute and compare the equations below:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "- If $P(Ham|w_1, w_2, ..., w_n) > P(Spam|w_1, w_2, ..., w_n)$, then the message is classified as ham.\n",
    "\n",
    "\n",
    "- If $P(Ham|w_1, w_2, ..., w_n) < P(Spam|w_1, w_2, ..., w_n)$, then the message is classified as spam.\n",
    "\n",
    "\n",
    "- If $P(Ham|w_1, w_2, ..., w_n) = P(Spam|w_1, w_2, ..., w_n)$, then the algorithm must request human help.\n",
    "\n",
    "To calculate $P(w_i|Spam)$ and $P(w_i|Ham)$ inside the formulas above, we need to calculate the two equations below, which read: 'the probability of a message that contains the word $w_i$ to be spam is...' and 'the probability of a message that contains the (same) word $w_i$ to be non spam is...', respectively. \n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "Here is where the training test enters as an input because we compute $P(w_i|Spam)$ and $P(w_i|Ham)$ based on elements taken and reformulated from this data set and then apply that to calculate $P(Spam | w_1,w_2, ..., w_n)$ and $P(Ham | w_1,w_2, ..., w_n)$. In other words, the Step 2 of the process is to create a 'blueprint' data set that allows us to compute\n",
    "$P(w_i|Spam)$ and $P(w_i|Ham)$ for every word of every message in the training data set, from which we pick up the probabilities associated with the words that are contained in a message that we wish to filter out.\n",
    "\n",
    "A mock example: \n",
    "\n",
    "We want to calculate this message probability of being spam: 'go crazy'.\n",
    "\n",
    "The probabilities in the 'blueprints' can be calculated because the word 'go' can be found in 667 normal messages and in 39 spam messages, while 'crazy' can be found in 8 normal messages and in 1 spam message (messages within the training set).\n",
    "\n",
    "The blueprints data sets can then be filled out:\n",
    "\n",
    "- Probability of spam given the targeted words:\n",
    "    - $P('go'|Spam) = a$\n",
    "    - $P('crazy'|Spam) = b$\n",
    "\n",
    "\n",
    "- Probability of spam given the targeted words:\n",
    "    - $P('go'|Ham) = c$\n",
    "    - $P('crazy'|Ham) = d$\n",
    "\n",
    "\n",
    "Which allow us to calculate:\n",
    "\n",
    "$P(Spam | w_1, w_2) = a * b$\n",
    "\n",
    "$P(Ham | w_1, w_2) = c *d$\n",
    "\n",
    "Having this information, we can compare values and determine if 'go crazy' is more likely to be, and ultimately categorized as spam or not spam.\n",
    "We can also notice that if a test message has a word which is not in the blueprint, because it wasn't included in the training set to begin with, it will not be considered in the filter, since we don't specify a way to calculate a probability for it. \n",
    "\n",
    "Let's also summarize what the terms in the equations above mean. Recall that all of this elements will be taken from the training set.\n",
    "\n",
    "\\begin{align}\n",
    "&N_{w_i|Spam} = \\text{the number of times the word } w_i \\text{ occurs in spam messages} \\\\\n",
    "&N_{w_i|Spam^C} = \\text{the number of times the word } w_i \\text{ occurs in non-spam messages} \\\\\n",
    "\\\\\n",
    "&N_{Spam} = \\text{total number of words in spam messages} \\\\\n",
    "&N_{Spam^C} = \\text{total number of words in non-spam messages} \\\\\n",
    "\\\\\n",
    "&N_{Vocabulary} = \\text{total number of words in the vocabulary} \\\\\n",
    "&\\alpha = 1 \\ \\ \\ \\ (\\alpha \\text{ is a smoothing parameter})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next stage we focus on working with the training set. In order to apply the Naive Bayes Theorem, we can build a table that, for each message, classifies it as spam or not spam and counts the number of times a word, from the vocabulary, appears in the message. Whenever a message does not contain a word from the vocabulary, each word in the vocabulary having its own column, it registers `0`. Remember that the vocabulary is the group of all unique words gathered from all the messages within the training set.\n",
    "\n",
    "Prior to building this DataFrame, two cleaning steps applied to the `SMS` column will be undertaken:\n",
    "\n",
    "- eliminate punctuation.\n",
    "- lower case every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set `SMS` cleaned series\n",
    "\n",
    "# Eliminating punctuation.\n",
    "cleaned = training_set.SMS.copy().str.replace('\\W', ' ', regex=True) \n",
    "\n",
    "# `{2.,}` ensures that if there are two or more joined whitespaces they are converted to just one.\n",
    "cleaned = cleaned.str.replace(' {2,}', ' ', regex=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last version of `cleaned` still has rows which have whitespaces at the beginning or end of the message which can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaned.str.replace('(\\A +| +\\Z)', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we confirm that there are no whitespaces at the beginning or end of the row/string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: SMS, dtype: object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond1 = cleaned.str.contains(pat='(?:\\A +| +\\Z)')\n",
    "\n",
    "cleaned[cond1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case for every string.\n",
    "cleaned = cleaned.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the changes in `cleaned` were successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                             yep by the pretty sculpture\n",
       "1                                                              yes princess are you going to make me moan\n",
       "2                                                                              welp apparently he retired\n",
       "3                                                                                                  havent\n",
       "4    i forgot 2 ask ü all smth there s a card on da present lei how ü all want 2 write smth or sign on it\n",
       "Name: SMS, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 4\n",
    "---\n",
    "On the previous screen, we removed the punctuation and changed all letters to lowercase. Recall that our end goal with this data cleaning process is to bring our training set to this format:\n",
    "\n",
    "![img_3](3.jpg)\n",
    "\n",
    "With the exception of the \"Label\" column, every other column in the transformed table above represents a unique word in our vocabulary (more specifically, each column shows the frequency of that unique word for any given message). Recall from the previous lesson that we call the set of unique words a **vocabulary**.\n",
    "\n",
    "We'll eventually bring the training set to that format ourselves, but first, let's create a list with all of the unique words that occur in the messages of our training set.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. Create a vocabulary for the messages in the training set. The vocabulary should be a Python list containing all the unique words across all messages, where each word is represented as a string.\n",
    "\n",
    "- Begin by transforming each message from the `SMS` column into a list by splitting the string at the space character — use the `Series.str.split()` method.\n",
    "\n",
    "\n",
    "- Initiate an empty list named `vocabulary`.\n",
    "\n",
    "\n",
    "- Iterate over the the `SMS` column (each message in this column should be a list of strings by the time you start this loop).\n",
    "    - Using a nested loop, iterate each message in the `SMS` column (each message should be a list of strings) and append each string (word) to the vocabulary list.\n",
    "    \n",
    "    \n",
    "- Transform the `vocabulary` list into a set using the `set()` function. This will remove the duplicates from the `vocabulary` list.\n",
    "\n",
    "\n",
    "- Transform the `vocabulary` set back into a list using the `list()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Producing a 'vocabulary' \n",
    "\n",
    "The next stage entails the production of a vocabulary (array of unique words taken from every message in `cleaned`), with the following steps:\n",
    "\n",
    "1. split each message into a list of words.\n",
    "2. create an empty set of unique words.\n",
    "3. add words to the set.\n",
    "4. convert set into back into a (sorted) list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "cleaned_split = cleaned.str.split(' ', expand=False) \n",
    "\n",
    "# 2.\n",
    "vocabulary = set()\n",
    "\n",
    "# 3.\n",
    "for index, sms in enumerate(cleaned_split):\n",
    "    for word in sms:\n",
    "        if word:\n",
    "            vocabulary.add(word)              \n",
    "            \n",
    "# 4.\n",
    "vocabulary = sorted(list(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When checking `vocabulary` below, we see that whitespace still appears as a value, thus it can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '00', '000', '000pes', '008704050406']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '00', '000', '000pes', '008704050406']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, el in enumerate(vocabulary):\n",
    "    if el == '':\n",
    "        del vocabulary[index]\n",
    "        \n",
    "vocabulary[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 5\n",
    "---\n",
    "On the previous screen, we managed to create the vocabulary for our messages in the training set. Now we're going to use the vocabulary to make the data transformation we need:\n",
    "\n",
    "![img_4](4.jpg)\n",
    "\n",
    "Eventually, we're going to create a new DataFrame. However, we'll first build a dictionary that we'll then convert to the DataFrame we need.\n",
    "\n",
    "For instance, to create the table we see above, we could use this dictionary and then convert it to a DataFrame:\n",
    "\n",
    "    word_counts_per_sms = {'secret': [2,1,1],\n",
    "                           'prize': [2,0,1],\n",
    "                           'claim': [1,0,1],\n",
    "                           'now': [1,0,1],\n",
    "                           'coming': [0,1,0],\n",
    "                           'to': [0,1,0],\n",
    "                           'my': [0,1,0],\n",
    "                           'party': [0,1,0],\n",
    "                           'winner': [0,0,1]\n",
    "                          }\n",
    "\n",
    "\n",
    "    word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "    word_counts.head()\n",
    "\n",
    "Output:\n",
    "\n",
    "|   |secret|prize|claim|now|coming|to |my |party|winner|\n",
    "|---|------|-----|-----|---|------|---|---|-----|------|\n",
    "|0  |2     |2    |1    |1  |0     |0  |0  |0    |0     |\n",
    "|1  |1     |0    |0    |0  |1     |1  |1  |1    |0     |\n",
    "|2  |1     |1    |1    |1  |0     |0  |0  |0    |1     |\n",
    "\n",
    "(As you may have noticed from the output above, the `Label` column is missing, but we'll get to that in the next exercise.)\n",
    "\n",
    "To create the dictionary we need for our training set, we can use the code below, where:\n",
    "\n",
    "- We start by initializing a dictionary named `word_counts_per_sms`, where each key is a unique word (a string) from the vocabulary, and each value is a list of the length of training set, where each element in the list is a `0`.\n",
    "    - The code `[0] * 5` outputs `[0, 0, 0, 0, 0]`. So the code `[0] * len(training_set['SMS']`) outputs a list of the length of `training_set['SMS']`, where each element in the list will be a `0`.\n",
    "\n",
    "\n",
    "- We loop over `training_set['SMS']` using at the same time the `enumerate()` function to get both the `index` and the SMS message (index and `sms`).\n",
    "    - Using a nested loop, we loop over `sms `(where `sms` is a list of strings, where each string represents a word in a message).\n",
    "        - We increment `word_counts_per_sms[word][index]` by `1`.\n",
    "\n",
    "---\n",
    "\n",
    "    word_counts_per_sms = {unique_word: [0] * \n",
    "        len(training_set['SMS']) for unique_word in vocabulary}\n",
    "\n",
    "    for index, sms in enumerate(training_set['SMS']):\n",
    "        for word in sms:\n",
    "            word_counts_per_sms[word][index] += 1\n",
    "\n",
    "Now that we have the dictionary we need, let's do the final transformations to our training set and then move forward with creating the spam filter.\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. Run the code you see above to get the `word_counts_per_sms` dictionary. In case you want to do a bit of exploration, note that this is a large dictionary, and printing it all is not recommended (you should rather use a for loop and print only the first five or so key-value pairs).\n",
    "\n",
    "\n",
    "2. Transform `word_counts_per_sms` into a DataFrame using `pd.DataFrame()`.\n",
    "\n",
    "\n",
    "3. Concatenate the DataFrame we just built above with the DataFrame containing the training set (this way, we'll also have the `Label` and the` SMS` columns). Use the `pd.concat()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Producing a word counter data set\n",
    "\n",
    "\n",
    "To continue the task of organizing the elements required to compute $P(w_i|Spam)$ and $P(w_i|Ham)$ for each word in the _vocabulary_, it is necessary to build a data set that registers how many times each word occurs in each message. To do this, we can collect this information by first compiling a dictionary and then convert it into a DataFrame, for the ease of read and access.\n",
    "\n",
    "Both the dictionary and DataFrame display the data similarly, we must be able to choose a word, that is represented by a key and a column, respectively, and be able to check the word frequency by message/row (in the dictionary each message is given by the index position in the list for every key/word). \n",
    "\n",
    "Before that we refine a little further the cleaning of messages:\n",
    "- Rows 1098 and 2700 are messages that only contain emojis, therefore they can be dropped out of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1098 ['']\n",
      "2700 ['']\n",
      "\n",
      "\n",
      "Label    ham\n",
      "SMS      :) \n",
      "Name: 1098, dtype: object\n",
      "Label        ham\n",
      "SMS      :-) :-)\n",
      "Name: 2700, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for index, value in enumerate(cleaned_split):\n",
    "    if '' in value:\n",
    "        print(index, value)\n",
    "\n",
    "print('\\nMessages containing only Emojis:\\n')       \n",
    "print(training_set.iloc[1098,:],'\\n')\n",
    "print(training_set.iloc[2700,:])\n",
    "\n",
    "cleaned_split = cleaned_split.drop([1098, 2700])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Filling out the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_per_sms = {unique_word: [0] * len(cleaned_split) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(cleaned_split):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hows does the dictionary looks like and how to read it:\n",
    "\n",
    "- We are looking at five random keys/unique words in the dictionary. Each index position of these lists represents one message, identical to the row index in `training_set`. Here we limited the search to the first 10 messages. Looking at this sample, we notice that none of these messages contains any of the following random words taken from the `vocabulary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert dictionary into DataFrame\n",
    "\n",
    "word_counts_per_sms_df = pd.DataFrame(word_counts_per_sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing if the conversion was successful: using the same example above, we can see that words are now column labels, whilst each message is represented by the (row) index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soc</th>\n",
       "      <th>gang</th>\n",
       "      <th>69888</th>\n",
       "      <th>suffers</th>\n",
       "      <th>edhae</th>\n",
       "      <th>he</th>\n",
       "      <th>costumes</th>\n",
       "      <th>hmv1</th>\n",
       "      <th>ummma</th>\n",
       "      <th>denying</th>\n",
       "      <th>...</th>\n",
       "      <th>away</th>\n",
       "      <th>karaoke</th>\n",
       "      <th>indian</th>\n",
       "      <th>schedule</th>\n",
       "      <th>tones2u</th>\n",
       "      <th>hurried</th>\n",
       "      <th>4th</th>\n",
       "      <th>infernal</th>\n",
       "      <th>verified</th>\n",
       "      <th>hot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 7775 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   soc  gang  69888  suffers  edhae  he  costumes  hmv1  ummma  denying  ...  \\\n",
       "0    0     0      0        0      0   0         0     0      0        0  ...   \n",
       "1    0     0      0        0      0   0         0     0      0        0  ...   \n",
       "\n",
       "   away  karaoke  indian  schedule  tones2u  hurried  4th  infernal  verified  \\\n",
       "0     0        0       0         0        0        0    0         0         0   \n",
       "1     0        0       0         0        0        0    0         0         0   \n",
       "\n",
       "   hot  \n",
       "0    0  \n",
       "1    0  \n",
       "\n",
       "[2 rows x 7775 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_per_sms_df.iloc[:2, 10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, concatenating `training_set` with `word_counts_per_sms_df` into a new DataFrame - `training_set_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. \n",
    "\n",
    "# `sort=False` is required to preserve the order of the columns in a 'first in' fashion.\n",
    "training_set_2 = pd.concat([training_set, word_counts_per_sms_df], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>dippeditinadew</th>\n",
       "      <th>edwards</th>\n",
       "      <th>shah</th>\n",
       "      <th>alaipayuthe</th>\n",
       "      <th>oranges</th>\n",
       "      <th>seem</th>\n",
       "      <th>piece</th>\n",
       "      <th>passed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yep, by the pretty sculpture</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes, princess. Are you going to make me moan?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                            SMS  dippeditinadew  \\\n",
       "0   ham                   Yep, by the pretty sculpture               0   \n",
       "1   ham  Yes, princess. Are you going to make me moan?               0   \n",
       "\n",
       "   edwards  shah  alaipayuthe  oranges  seem  piece  passed  \n",
       "0        0     0            0        0     0      0       0  \n",
       "1        0     0            0        0     0      0       0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_2.iloc[:2, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing whether the rows in `training_set` were well aligned with the correspondent rows in `word_counts_per_sms_df`, two examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: row 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set_2[\"SMS\"], row 0: Yep, by the pretty sculpture\n",
      "\n",
      "\n",
      "training_set_2, row 0 - columns that are \"1 or more\":\n",
      "\n",
      "sculpture    1\n",
      "the          1\n",
      "pretty       1\n",
      "by           1\n",
      "yep          1\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cond_row_0 = training_set_2.iloc[0, 2:] == 1 \n",
    "\n",
    "print('training_set_2[\"SMS\"], row 0: {}'.format(training_set_2.iloc[0, 1]))\n",
    "print('\\n')\n",
    "print('training_set_2, row 0 - columns that are \"1 or more\":\\n\\n{}'.format(training_set_2.iloc[0, 2:][cond_row_0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: row 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, princess. Are you going to make me moan?\n",
      "\n",
      "\n",
      "going       1\n",
      "you         1\n",
      "make        1\n",
      "yes         1\n",
      "are         1\n",
      "princess    1\n",
      "moan        1\n",
      "to          1\n",
      "me          1\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cond_row_1 = training_set_2.iloc[1, 2:] == 1 \n",
    "\n",
    "print(training_set_2.iloc[1, 1])\n",
    "print('\\n')\n",
    "print(training_set_2.iloc[1, 2:][cond_row_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free RAM 2\n",
    "del word_counts_per_sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 6\n",
    "---\n",
    "Now that we're done with data cleaning and have a training set to work with, we can begin creating the spam filter. Recall that the Naive Bayes algorithm will need to know the probability values of the two equations below to be able to classify new messages:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam) \\\\\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "Also, to calculate $P(w_i|Spam)$ and $P(w_i|Ham)$ inside the formulas above, recall that we need to use these equations:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}} \\\\\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Some of the terms in the four equations above will have the same value for every new message. As a start, let's first calculate:\n",
    "\n",
    "- $P(Spam)$ and $P(Ham)$.\n",
    "\n",
    "\n",
    "- $N_{Spam}$, $N_{Ham}$, $N_{Vocabulary}$.\n",
    "\n",
    "\n",
    "Recall from the previous lesson that:\n",
    "\n",
    "- $N_{Spam}$ is equal to the number of words in all the spam messages — it's not equal to the number of spam messages, and it's _not_ equal to the total number of _unique_ words in spam messages.\n",
    "\n",
    "\n",
    "- $N_{Ham}$ is equal to the number of words in all the non-spam messages — it's _not_ equal to the number of non-spam messages, and it's not equal to the total number of _unique_ words in non-spam messages.\n",
    "\n",
    "- We'll also use Laplace smoothing and set $α=1$.\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. Calculate P(Spam) and P(Ham). There's more than one way to write the code that can calculate this — feel free to choose any solution you want.\n",
    "\n",
    "\n",
    "2. Calculate $N_{Spam}$, $N_{Ham}$, $N_{Vocabulary}$. Feel free to choose any programming solution you like.\n",
    "\n",
    "\n",
    "3. Initiate a variable named alpha with a value of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the elements within the Naive Bayes algorithm. Starting with:\n",
    "\n",
    "- $P(Spam)$ and $P(Ham)$.\n",
    "- $N_{Spam}$, $N_{Ham}$, $N_{Vocabulary}$\n",
    "\n",
    "The Laplace smoothing parameter is set to 1: $α=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of Spam and Ham. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_ts2 = training_set_2.Label.value_counts(normalize=True)\n",
    "\n",
    "p_spam = label_counts_ts2.spam\n",
    "\n",
    "p_ham = label_counts_ts2.ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13455931823278763"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8654406817672123"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting $N_{Spam}$, $N_{Ham}$, $N_{Vocabulary}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7785"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_vocabulary.\n",
    "\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "n_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up the total number of words for spam messages and for non-spam messages - $N_{Spam}$, $N_{Ham}$ respectively, the procedure will be the following (we count repeats):\n",
    "\n",
    "1. add a column summing up the number of words per message/row.\n",
    "\n",
    "2. calculate sum of words if row if messages are spam; same for non-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "training_set_2['sum_words_sms'] = training_set_2.iloc[:, 2:].copy().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "\n",
    "n_spam = training_set_2.loc[training_set_2.Label=='spam', 'sum_words_sms'].sum()\n",
    "\n",
    "n_ham = training_set_2.loc[training_set_2.Label=='ham', 'sum_words_sms'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_spam: 15190 \n",
      "n_ham: 57246\n"
     ]
    }
   ],
   "source": [
    "print(f'n_spam: {n_spam}',\n",
    "     f'\\nn_ham: {n_ham}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally setting $α=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 7\n",
    "---\n",
    "On the previous screen, we managed to calculate a few terms for our equations:\n",
    "\n",
    "- $P(Spam)$ and $P(Ham)$.\n",
    "\n",
    "\n",
    "- $N_{Spam}$, $N_{Ham}$, $N_{Vocabulary}$.\n",
    "\n",
    "\n",
    "As we've already mentioned, all these terms will have constant values in our equations for every new message (regardless of the message or each individual word in the message).\n",
    "\n",
    "However, $P(w_i|Spam)$ and $P(w_i|Ham)$ will vary depending on the individual words. For instance, P(\"secret\"|Spam) will have a certain probability value, while P(\"cousin\"|Spam) or P(\"lovely\"|Spam) will most likely have other values.\n",
    "\n",
    "Although both $P(w_i|Spam)$ and $P(w_i|Ham)$ vary depending on the word, the probability for each individual word is constant for every new message.\n",
    "\n",
    "\n",
    "\n",
    "For instance, let's say we receive two new messages:\n",
    "\n",
    "\n",
    "- \"secret code\".\n",
    "\n",
    "\n",
    "- \"secret party 2night\".\n",
    "\n",
    "\n",
    "We'll need to calculate P(\"secret\"|Spam) for both these messages, and we can use the training set to get the values we need to find a result for the equation below:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\text{\"secret\"}|Spam) = \\frac{N_{\"secret\"|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "The steps we take to calculate P(\"secret\"|Spam) will be identical for both of our new messages above, or for any other new message that contains the word \"secret\". The key detail here is that calculating P(\"secret\"|Spam) only depends on the training set, and as long as we don't make changes to the training set, P(\"secret\"|Spam) stays constant. The same reasoning also applies to P(\"secret\"|Ham).\n",
    "\n",
    "This means that we can use our training set to calculate the probability for each word in our vocabulary. If our vocabulary contained only the words \"lost\", \"navigate\", and \"sea\", then we'd need to calculate six probabilities:\n",
    "\n",
    "\n",
    "- P(\"lost\"|Spam) and P(\"lost\"|Ham)\n",
    "\n",
    "\n",
    "- P(\"navigate\"|Spam) and P(\"navigate\"|Ham)\n",
    "\n",
    "\n",
    "- P(\"sea\"|Spam) and P(\"sea\"|Ham)\n",
    "\n",
    "\n",
    "We have 7,783 words in our vocabulary, which means we'll need to calculate a total of 15,566 probabilities. For each word, we need to calculate both $P(w_i|Spam)$ and $P(w_i|Ham)$.\n",
    "\n",
    "In more technical language, the probability values that $P(w_i|Spam)$ and $P(w_i|Ham)$ will take are called **parameters**.\n",
    "\n",
    "The fact that we calculate so many values before even beginning the classification of new messages makes the Naive Bayes algorithm very fast (especially compared to other algorithms). When a new message comes in, most of the needed computations are already done, which enables the algorithm to almost instantly classify the new message.\n",
    "\n",
    "If we didn't calculate all these values beforehand, then all these calculations would need to be done every time a new message comes in. Imagine the algorithm will be used to classify 1,000,000 new messages. Why repeat all these calculations 1,000,000 times when we could just do them once at the beginning?\n",
    "\n",
    "Let's now calculate all the parameters using the equations below:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}} = \\frac{\\text{(number of times $w_i$ appears in spam messages)} + 1}{\\text{(total number of spam words)} + (1 * \\text{total number of unique words})} \\\\\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}} = \\frac{\\text{(number of times $w_i$ appears in ham messages)} + 1}{\\text{(total number of ham words)} + (1 * \\text{total number of unique words})} \\\\\n",
    "\\end{equation}\n",
    "\n",
    "Recall that $P(w_i|Spam)$ and $P(w_i|Ham)$ are key parts of the equations that we need to classify the new messages:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam) \\\\\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "\n",
    "\n",
    "1. Initialize two dictionaries, where each key-value pair is a unique word (from our vocabulary) represented as a string, and the value is `0`. We'll need one dictionary to store the parameters for $P(w_i|Spam)$, and the other for $P(w_i|Ham)$.\n",
    "    - If the entire vocabulary were `['sea', 'navigate']`, we'd need to initialize two dictionaries, one for spam and one for ham, and both should look like this: `{'sea': 0, 'navigate': 0}`.\n",
    "\n",
    "\n",
    "2. Isolate the spam and the ham messages in the training set into two different DataFrames. The `Label` column will help you isolate the messages.\n",
    "\n",
    "3. Iterate over the vocabulary, and, for each word, calculate $P(w_i|Spam)$ and $P(w_i|Ham)$ using the formulas we mentioned above.\n",
    "    - Recall that $N_{Spam}$, $N_{Ham}$, $N_{Vocabulary}$, and $α$ are already calculated from the last screen.\n",
    "    \n",
    "    - Recall from the previous lesson that $N_{w_i|Spam}$ is equal to the number of times the word $w_i$ occurs in all the spam messages, while $N_{w_i|Ham}$ is equal to the number of times the word $w_i$ occurs in all the ham messages.\n",
    "    \n",
    "- Once you're done with calculating an individual parameter, update the probability value in the two dictionaries you created initially.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To arrive to the dictionaries that contain every $P(w_i|Spam)$ and $P(w_i|Ham)$, I will do the following:\n",
    "\n",
    "\n",
    "First, we create two Series that gives us $N_{w_i|Spam}$ and $N_{w_i|Ham}$:\n",
    "\n",
    "1. create two DataFrames that only contain either spam or non-spam messages.\n",
    "\n",
    "\n",
    "2. out of those two DataFrames create two correspondent Series - `sms_spam_sum` and `sms_ham_sum`; analogously for both Series, the row index is the column index of the former DataFrame, and each value of the Series corresponds to the sum of the values in each column. For a more intuitive understanding, see below the transformations made in `sms_spam`.\n",
    "\n",
    "\n",
    "3. the last step before building the final spam filter is to fill out two dictionaries, that store $P(w_i|Spam)$ and $P(w_i|Ham)$ for each word in the _vocabulary_, based on the elements already collected:\n",
    "\n",
    "    - `n_spam` and `n_ham`.\n",
    "    - `n_vocabulary`.\n",
    "    - $N_{w_i|Spam}$ and $N_{w_i|Ham}$.\n",
    "    - `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. the last colum 'sum_words_sms' is not included in the calculations\n",
    "# of these Series, so I set `.iloc[:, 2:-1]`.\n",
    "sms_spam = training_set_2.iloc[:, 2:-1].copy()[training_set_2.Label=='spam']\n",
    "\n",
    "sms_ham = training_set_2.iloc[:, 2:-1].copy()[training_set_2.Label=='ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dippeditinadew</th>\n",
       "      <th>edwards</th>\n",
       "      <th>shah</th>\n",
       "      <th>alaipayuthe</th>\n",
       "      <th>oranges</th>\n",
       "      <th>seem</th>\n",
       "      <th>piece</th>\n",
       "      <th>passed</th>\n",
       "      <th>november</th>\n",
       "      <th>names</th>\n",
       "      <th>...</th>\n",
       "      <th>away</th>\n",
       "      <th>karaoke</th>\n",
       "      <th>indian</th>\n",
       "      <th>schedule</th>\n",
       "      <th>tones2u</th>\n",
       "      <th>hurried</th>\n",
       "      <th>4th</th>\n",
       "      <th>infernal</th>\n",
       "      <th>verified</th>\n",
       "      <th>hot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 7785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dippeditinadew  edwards  shah  alaipayuthe  oranges  seem  piece  passed  \\\n",
       "16               0        0     0            0        0     0      0       0   \n",
       "18               0        0     0            0        0     0      0       0   \n",
       "56               0        0     0            0        0     0      0       0   \n",
       "\n",
       "    november  names  ...  away  karaoke  indian  schedule  tones2u  hurried  \\\n",
       "16         0      0  ...     0        0       0         0        0        0   \n",
       "18         0      0  ...     0        0       0         0        0        0   \n",
       "56         0      0  ...     0        0       0         0        0        0   \n",
       "\n",
       "    4th  infernal  verified  hot  \n",
       "16    0         0         0    0  \n",
       "18    0         0         0    0  \n",
       "56    0         0         0    0  \n",
       "\n",
       "[3 rows x 7785 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_spam.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "sms_spam_sum = sms_spam.sum().transpose()\n",
    "\n",
    "sms_ham_sum = sms_ham.sum().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dippeditinadew    0\n",
       "edwards           0\n",
       "shah              0\n",
       "alaipayuthe       0\n",
       "oranges           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_spam_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "\n",
    "p_wi_given_spam_dict = {}\n",
    "\n",
    "p_wi_given_ham_dict = {}\n",
    "\n",
    "# P(w_i|Spam)\n",
    "for i in range(0, sms_spam_sum.size):\n",
    "    index = sms_spam_sum.index[i]\n",
    "    dividend = sms_spam_sum[index] + alpha\n",
    "    divisor = n_spam + (alpha*n_vocabulary)\n",
    "    p_wi_given_spam_dict[index] =  dividend / divisor\n",
    "\n",
    "    \n",
    "# P(w_i|Ham)\n",
    "for i in range(0, sms_ham_sum.size):\n",
    "    index = sms_ham_sum.index[i]\n",
    "    dividend = sms_ham_sum[index] + alpha\n",
    "    divisor = n_ham + (alpha*n_vocabulary)\n",
    "    p_wi_given_ham_dict[index] =  dividend / divisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking first 10 items in `p_wi_given_spam_dict` and `p_wi_given_ham_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dippeditinadew', 4.352557127312296e-05),\n",
       " ('edwards', 4.352557127312296e-05),\n",
       " ('shah', 4.352557127312296e-05),\n",
       " ('alaipayuthe', 4.352557127312296e-05),\n",
       " ('oranges', 8.705114254624592e-05),\n",
       " ('seem', 8.705114254624592e-05),\n",
       " ('piece', 4.352557127312296e-05),\n",
       " ('passed', 4.352557127312296e-05),\n",
       " ('november', 8.705114254624592e-05),\n",
       " ('names', 0.00013057671381936888)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(p_wi_given_spam_dict.items())[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dippeditinadew', 4.613184481247405e-05),\n",
       " ('edwards', 3.0754563208316035e-05),\n",
       " ('shah', 3.0754563208316035e-05),\n",
       " ('alaipayuthe', 4.613184481247405e-05),\n",
       " ('oranges', 1.5377281604158017e-05),\n",
       " ('seem', 4.613184481247405e-05),\n",
       " ('piece', 4.613184481247405e-05),\n",
       " ('passed', 6.150912641663207e-05),\n",
       " ('november', 1.5377281604158017e-05),\n",
       " ('names', 3.0754563208316035e-05)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(p_wi_given_ham_dict.items())[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free RAM 4\n",
    "\n",
    "del sms_spam\n",
    "del sms_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 8\n",
    "---\n",
    "Now that we've calculated all the constants and parameters we need, we can start creating the spam filter. The spam filter can be understood as a function that:\n",
    "\n",
    "- Takes in as input a new message ($w_1$, $w_2$, ..., $w_n$).\n",
    "\n",
    "\n",
    "- Calculates \n",
    ".\n",
    "  \n",
    "  \n",
    "- Compares the values of $P(Spam|w_1, w_2, ..., w_n)$ and $P(Ham|w_1, w_2, ..., w_n)$, and:\n",
    "    - If $P(Ham|w_1, w_2, ..., w_n) > P(Spam|w_1, w_2, ..., w_n)$, then the message is classified as ham.\n",
    "    \n",
    "    - If $P(Ham|w_1, w_2, ..., w_n) < P(Spam|w_1, w_2, ..., w_n)$, then the message is classified as spam.\n",
    "    \n",
    "    - If $P(Ham|w_1, w_2, ..., w_n) = P(Spam|w_1, w_2, ..., w_n)$, then the algorithm may request human help.\n",
    "\n",
    "Below, we see a rough sketch of how the spam filter function might look like:\n",
    "\n",
    "    import re\n",
    "\n",
    "    def classify(message):\n",
    "\n",
    "        message = re.sub('\\W', ' ', message)\n",
    "        message = message.lower()\n",
    "        message = message.split()\n",
    "\n",
    "        '''    \n",
    "        This is where we calculate:\n",
    "\n",
    "        p_spam_given_message = ?\n",
    "        p_ham_given_message = ?\n",
    "        '''    \n",
    "\n",
    "        print('P(Spam|message):', p_spam_given_message)\n",
    "        print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "        if p_ham_given_message > p_spam_given_message:\n",
    "            print('Label: Ham')\n",
    "        elif p_ham_given_message < p_spam_given_message:\n",
    "            print('Label: Spam')\n",
    "        else:\n",
    "            print('Equal proabilities, have a human classify this!')\n",
    "\n",
    "\n",
    "\n",
    "For the `classify()` function above, note that:\n",
    "\n",
    "\n",
    "- The input variable `message` is assumed to be a string.\n",
    "\n",
    "\n",
    "- We perform a bit of data cleaning on the string `message`:\n",
    "    - We remove the punctuation using the `re.sub()` function.\n",
    "    - We bring all letters to lower case using the `str.lower()` method.\n",
    "    - We split the string at the space character and transform it into a Python list using the `str.split()` method.\n",
    "    \n",
    "    \n",
    "- There's some placeholder code for calculating `p_spam_given_message` and `p_ham_given_message` — we'll write this code in the exercise below.\n",
    "\n",
    "\n",
    "- We compare `p_spam_given_message` with p_ham_given_message and then print a classification label.\n",
    "\n",
    "To write the code we need for calculating `p_spam_given_message` and `p_ham_given_message`, we need to use these two equations:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "Note that some new messages will contain words that are not part of the vocabulary. Recall from the previous lesson that we simply ignore these words when we're calculating the probabilities.\n",
    "\n",
    "Now we'll write the code for calculating `p_spam_given_message` and `p_ham_given_message`, and then we'll use the function to classify two new messages. On the next screen, we'll classify all the 1,114 messages in our test set.\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. Copy the `classify()` function you see above and write the code needed for calculating `p_spam_given_message` and `p_ham_given_message`.\n",
    "    - Initiate p_spam_given_message and p_ham_given_message with an initial value. We recommend initiating the variables as p_spam_given_message = p_spam and p_ham_given_message = p_ham (p_spam and p_ham are P(Spam) and P(Ham), and they were calculated on the previous steps).\n",
    "    - Iterate over each word in `message` (the input of the `classify()` function), which should be a list of strings by the time you start this loop. For each word:\n",
    "        - If the word is present in the dictionary containing the spam parameters, then update the value of `p_spam_given_message` by multiplying with the parameter value specific to that word. You'll need to code something similar to `p_spam_given_message *= parameters_spam[word]`.\n",
    "        - If the word is present in the dictionary containing the ham parameters, then update the value of `p_ham_given_message` by multiplying with the parameter value specific to that word. You'll need to do something like `p_ham_given_message *= parameters_spam[word]`.\n",
    "        - If the word is not present in any of the two dictionaries, then don't do anything. Recall that we ignore words that are not part of the vocabulary.\n",
    "\n",
    "\n",
    "\n",
    "3. Use the `classify()` function to classify two new messages. You can use any messages you want, but we suggest that one message is obviously spam, and the other is obviously ham. For instance, you can use these two messages:\n",
    "    - 'WINNER!! This is the secret code to unlock the money: C3421.'\n",
    "    - \"Sounds good, Tom, then see u there\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Up All Parts Together - Building The Spam Filter\n",
    "---\n",
    "\n",
    "Now that $P(w_i|Spam)$ and $P(w_i|Ham)$ have been calculated throughout the entire span of messages contained in the training set, it is possible to finally build the spam filter by calculating and comparing $P(Spam|w_1, w_2, ..., w_n)$ with $P(Ham|w_1, w_2, ..., w_n)$. If by chance the values calculated for these probabilities are equal, no classification is made, and the function returns a string message asking for a human classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message):\n",
    "    \"\"\"Takes in a string - a cellphone message (SMS), and returns the probability of Spam given the input message,\n",
    "    the probability of non-spam (ham) given the input message and classifies whether\n",
    "    the message is spam, not spam (ham), or if a human is required to classify the message.\n",
    "    \"\"\"\n",
    "\n",
    "    message = re.sub('\\W', ' ', message) # still a string\n",
    "    message = message.lower() # still a string\n",
    "    message = message.split() # now a list of strings\n",
    "\n",
    "    \n",
    "    # Calculating P(Spam|w_1, w_2, ..., w_n) with P(Ham|w_1, w_2, ..., w_n).\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    # Note: if `word` is not in the spam or in the non-spam DataFrames the loop does nothing by default.\n",
    "    for word in message:\n",
    "        \n",
    "        if word in p_wi_given_spam_dict.keys():\n",
    "            p_spam_given_message *= p_wi_given_spam_dict[word]\n",
    "            \n",
    "        if word in p_wi_given_ham_dict.keys():\n",
    "            p_ham_given_message *= p_wi_given_ham_dict[word]\n",
    "        \n",
    "\n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "        \n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "        \n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 1.3467710812047665e-25\n",
      "P(Ham|message): 1.9339258494902383e-27\n",
      "Label: Spam\n"
     ]
    }
   ],
   "source": [
    "classify('WINNER!! This is the secret code to unlock the money: C3421.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 2.43520654878629e-25\n",
      "P(Ham|message): 3.6832948885668876e-21\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "classify('Sounds good, Tom, then see u there')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 9\n",
    "---\n",
    "On the previous screen, we managed to create a spam filter, and we classified two new messages. We'll now try to determine how well the spam filter does on our test set of 1,114 messages.\n",
    "\n",
    "The algorithm will output a classification label for every message in our test set, which we'll be able to compare with the actual label (given by a human). Note that, in training, our algorithm didn't see these 1,114 messages, so every message in the test set is practically new from the perspective of the algorithm.\n",
    "\n",
    "First off, we'll change the `classify()` function that we wrote previously to return the labels instead of printing them. Below, note that we now have `return` statements instead of `print()` functions:\n",
    "\n",
    "    def classify_test_set(message):\n",
    "\n",
    "        message = re.sub('\\W', ' ', message)\n",
    "        message = message.lower()\n",
    "        message = message.split()\n",
    "\n",
    "        p_spam_given_message = p_spam\n",
    "        p_ham_given_message = p_ham\n",
    "\n",
    "        for word in message:\n",
    "            if word in parameters_spam:\n",
    "                p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "            if word in parameters_ham:\n",
    "                p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "        if p_ham_given_message > p_spam_given_message:\n",
    "            return 'ham'\n",
    "        elif p_spam_given_message > p_ham_given_message:\n",
    "            return 'spam'\n",
    "        else:\n",
    "            return 'needs human classification'\n",
    "\n",
    "Now that we have a function that returns labels instead of printing them, we can use it to create a new column in our test set.\n",
    "\n",
    "    test_set['predicted'] = test_set['SMS'].apply(classify_test_set)\n",
    "    test_set.head()\n",
    "\n",
    "Output:\n",
    "\n",
    "|   |Label|SMS                                               |predicted|\n",
    "|---|-----|--------------------------------------------------|---------|\n",
    "|0  |ham  |Later i guess. I needa do mcat study too.         |ham      |\n",
    "|1  |ham  |But i haf enuff space got like 4 mb...            |ham      |\n",
    "|2  |spam |Had your mobile 10 mths? Update to latest Oran... |spam     |\n",
    "|3  |ham  |All sounds good. Fingers . Makes it difficult ... |ham      |\n",
    "|4  |ham  |All done, all handed in. Don't know if mega sh... |ham      |\n",
    "\n",
    "\n",
    "Now we can compare the predicted values with the actual values to measure how good our spam filter is with classifying new messages. To make the measurement, we'll use **accuracy** as a metric:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Accuracy} = \\frac{\\text{number of correctly classified messages}}{\\text{total number of classified messages}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. Measure the accuracy of the spam filter.\n",
    "- Initialize a variable named correct with a value of `0`.\n",
    "- Initialize a variable named `total` with the number of messages in the test set.\n",
    "- Iterate over the test set DataFrame (you can use the `DataFrame.iterrows() method`). For each row:\n",
    "    - If the actual label is the same as the predicted label, then increment `correct` by `1`.\n",
    "    - Use `correct` and `total` in combination with the above formula to calculate the accuracy of the spam filter.\n",
    "\n",
    "2. What do you think about the accuracy value? Is it better or worse than you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of going for the suggested method in the instructions we'll add two new columns to `testing_set`: one, `Test`, displays the output returned by `classify_test_set()` for each row/message; the last, `Correct` is '1' if the filter returns the right classification, i.e. the same classification in the `Label` column (which previously classifies the message has 'spam' or 'ham'), or '0' otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "    \"\"\"Takes in a string - a cellphone message (SMS), and returns a classification of whether \n",
    "    the message is spam, not spam (ham), or if a human is required to classify the message.\n",
    "    \"\"\"\n",
    "\n",
    "    message = re.sub('\\W', ' ', message) # still a string\n",
    "    message = message.lower() # still a string\n",
    "    message = message.split() # now a list of strings\n",
    "\n",
    "    \n",
    "    # Calculating P(Spam|w_1, w_2, ..., w_n) with P(Ham|w_1, w_2, ..., w_n).\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    # Note: if `word` is not in the spam or in the non-spam DataFrames the loop does nothing by default.\n",
    "    for word in message:\n",
    "        \n",
    "        if word in p_wi_given_spam_dict.keys():\n",
    "            p_spam_given_message *= p_wi_given_spam_dict[word]\n",
    "            \n",
    "        if word in p_wi_given_ham_dict.keys():\n",
    "            p_ham_given_message *= p_wi_given_ham_dict[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'Requires human classification.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are in the conditions to apply the filter to every message on the `testing_set`. We register if the filter was successful in classifying each message correctly by comparing it with the classification in the `Label` column. We store these results in a column named `Correct` where '1' means well classified, and '0' wrongly classified. This will allow to easily calculate the accuracy rate of the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set['Test'] = testing_set['SMS'].apply(classify_test_set)\n",
    "\n",
    "\n",
    "# Assigns True if condition is met and multplying by one converts True in '1' and False in '0'.\n",
    "testing_set['Correct'] = (testing_set['Label'] == testing_set['Test'])*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>Test</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4458</th>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "      <td>ham</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4459</th>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4460</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4462</th>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                                SMS  Test  Correct\n",
       "4458   ham          Later i guess. I needa do mcat study too.   ham        1\n",
       "4459   ham             But i haf enuff space got like 4 mb...   ham        1\n",
       "4460  spam  Had your mobile 10 mths? Update to latest Oran...  spam        1\n",
       "4461   ham  All sounds good. Fingers . Makes it difficult ...   ham        1\n",
       "4462   ham  All done, all handed in. Don't know if mega sh...   ham        1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of well classified messages when using the `classify_test_set()` function/filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When applied to the messages in the `testing_set` (1114 entries) the test accuracy was aprox. 98.74%.\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = (testing_set['Correct'].sum() / testing_set.shape[0])*100\n",
    "\n",
    "test_accuracy = test_accuracy.round(2)\n",
    "\n",
    "print(f'When applied to the messages in the `testing_set` ({testing_set.shape[0]} entries) the test accuracy was aprox. {test_accuracy}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the value of the test accuracy of 98.74%, we can classify it as a success, since it exceeded the approval threshold of 80% by a substantial margin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 10 (end of project)\n",
    "---\n",
    "In this project, we managed to build a spam filter for SMS messages using the multinomial Naive Bayes algorithm. The filter had an accuracy of 98.74% on the test set, which is an excellent result. We initially aimed for an accuracy of over 80%, but we managed to do way better than that.\n",
    "\n",
    "If you want to keep working on this project, here's a few next steps you can take:\n",
    "\n",
    "\n",
    "1. Isolate the 14 messages that were classified incorrectly and try to figure out why the algorithm reached the wrong conclusions.\n",
    "\n",
    "\n",
    "2. Make the filtering process more complex by making the algorithm sensitive to letter case.\n",
    "\n",
    "\n",
    "3. Get the project portfolio-ready by using a few tips from our style guide for data science projects.\n",
    "\n",
    "\n",
    "Congratulations, this is the end of the Conditional Probability course! We've come a long way and learned how to:\n",
    "\n",
    "- Assign probabilities to events based on certain conditions by using conditional probability rules.\n",
    "\n",
    "\n",
    "- Assign probabilities to events based on whether they are in relationship of statistical independence or not with other events.\n",
    "\n",
    "\n",
    "- Assign probabilities to events based on prior knowledge by using Bayes' theorem.\n",
    "\n",
    "\n",
    "- Create a spam filter for SMS messages using the multinomial Naive Bayes algorithm.\n",
    "\n",
    "Curious to see what other students have done on this project? [Head over to our Community to check them out](https://community.dataquest.io/tags/c/social/share/49/433). While you are there, please remember to show some love and give your own feedback!\n",
    "\n",
    "And of course, we welcome you to share your own project and show off your hard work. Head over to our Community to [share your finished Guided Project](https://community.dataquest.io/tags/c/social/share/49/433)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Isolate the 14 messages that were classified incorrectly and try to figure out why the algorithm reached the wrong conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, lets observe closer the 14 messages that were incorrectly tagged. For that purpose we isolate those messages into a separate DataFrame (`incorrect`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_incorrect = testing_set.Correct == 0\n",
    "\n",
    "incorrect = testing_set[cond_incorrect].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic info on `incorrect`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14 entries, 0 to 13\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Label    14 non-null     object\n",
      " 1   SMS      14 non-null     object\n",
      " 2   Test     14 non-null     object\n",
      " 3   Correct  14 non-null     int32 \n",
      "dtypes: int32(1), object(3)\n",
      "memory usage: 288.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "incorrect.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Not heard from U4 a while. Call me now am here all night with just my knickers on. Make me beg for it like U did last time 01223585236 XX Luv Nikiyu4.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>More people are dogging in your area now. Call 09090204448 and join like minded guys. Why not arrange 1 yourself. There's 1 this evening. A£1.50 minAPN LS278BB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Unlimited texts. Limited minutes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>26th OF JULY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nokia phone is lovly..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>A Boy loved a gal. He propsd bt she didnt mind. He gv lv lttrs, Bt her frnds threw thm. Again d boy decided 2 aproach d gal , dt time a truck was speeding towards d gal. Wn it was about 2 hit d girl,d boy ran like hell n saved her. She asked 'hw cn u run so fast?' D boy replied \"Boost is d secret of my energy\" n instantly d girl shouted \"our energy\" n Thy lived happily 2gthr drinking boost evrydy Moral of d story:- I hv free msgs:D;): gud ni8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>No calls..messages..missed calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hi babe its Chloe, how r u? I was smashed on saturday night, it was great! How was your weekend? U been missing me? SP visionsms.com Text stop to stop 150p/text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spam</td>\n",
       "      <td>0A$NETWORKS allow companies to bill for SMS, so they are responsible for their \"suppliers\", just as a shop has to give a guarantee on what they sell. B. G.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>RCT' THNQ Adrian for U text. Rgds Vatian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>2/2 146tf150p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hello. We need some posh birds and chaps to user trial prods for champneys. Can i put you down? I need your address and dob asap. Ta r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  \\\n",
       "0   spam   \n",
       "1   spam   \n",
       "2    ham   \n",
       "3    ham   \n",
       "4    ham   \n",
       "5    ham   \n",
       "6    ham   \n",
       "7    ham   \n",
       "8   spam   \n",
       "9   spam   \n",
       "10  spam   \n",
       "11  spam   \n",
       "12  spam   \n",
       "13  spam   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                               SMS  \n",
       "0                                                                                                                                                                                                                                                                                                        Not heard from U4 a while. Call me now am here all night with just my knickers on. Make me beg for it like U did last time 01223585236 XX Luv Nikiyu4.net  \n",
       "1                                                                                                                                                                                                                                                                                                  More people are dogging in your area now. Call 09090204448 and join like minded guys. Why not arrange 1 yourself. There's 1 this evening. A£1.50 minAPN LS278BB  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                Unlimited texts. Limited minutes.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                     26th OF JULY  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                           Nokia phone is lovly..  \n",
       "5   A Boy loved a gal. He propsd bt she didnt mind. He gv lv lttrs, Bt her frnds threw thm. Again d boy decided 2 aproach d gal , dt time a truck was speeding towards d gal. Wn it was about 2 hit d girl,d boy ran like hell n saved her. She asked 'hw cn u run so fast?' D boy replied \"Boost is d secret of my energy\" n instantly d girl shouted \"our energy\" n Thy lived happily 2gthr drinking boost evrydy Moral of d story:- I hv free msgs:D;): gud ni8  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                 No calls..messages..missed calls  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us  \n",
       "8                                                                                                                                                                                                                                                                                                                                                       Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50  \n",
       "9                                                                                                                                                                                                                                                                                                 Hi babe its Chloe, how r u? I was smashed on saturday night, it was great! How was your weekend? U been missing me? SP visionsms.com Text stop to stop 150p/text  \n",
       "10                                                                                                                                                                                                                                                                                                     0A$NETWORKS allow companies to bill for SMS, so they are responsible for their \"suppliers\", just as a shop has to give a guarantee on what they sell. B. G.  \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                        RCT' THNQ Adrian for U text. Rgds Vatian  \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                   2/2 146tf150p  \n",
       "13                                                                                                                                                                                                                                                                                                                          Hello. We need some posh birds and chaps to user trial prods for champneys. Can i put you down? I need your address and dob asap. Ta r  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pd.options.display.max_colwidth = 500\n",
    "\n",
    "incorrect[['Label', 'SMS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we can read above, we must first acknowledge that some messages are too ambiguous and difficult to pronounce correctly as spam or not spam, even when resorting to human judgment, some examples: \n",
    "\n",
    "- 'Unlimited texts. Limited minutes.' (row 2).\n",
    "- 'We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us' (row 7).\n",
    "- 'RCT' THNQ Adrian for U text. Rgds Vatian' (row 11).\n",
    "\n",
    "One thing we can observe in `incorrect` is that there are spam messages that have implicit references to money or currency symbols like '£' or '150p', where this last expression stands for 150 pence (same as 1.5£). In the case of currency symbols, when we trained the the algorithm we excluded them when we set this piece of code previously:\n",
    "\n",
    "    # Training set `SMS` cleaned series\n",
    "\n",
    "    # Eliminating punctuation.\n",
    "    cleaned = training_set.SMS.copy().str.replace('\\W', ' ', regex=True) \n",
    "\n",
    "Because we chose to replace an undiscriminated group of characters by setting `pattern='\\W'`, we prevented the algorithm from recognizing (at least some direct) money references, which are common in spam messages. Therefore, one way to improve the algorithm is to make it recognize currency or money expressions. \n",
    "\n",
    "\n",
    "For the sake of comprehension, we can find below all the suppressed characters in `training_set.SMS` which correspond to the regex character class 'not word' ('\\W'): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t',\n",
       " '\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '|',\n",
       " '~',\n",
       " '\\x91',\n",
       " '\\x92',\n",
       " '\\x93',\n",
       " '\\x94',\n",
       " '\\x96',\n",
       " '¡',\n",
       " '£',\n",
       " '»',\n",
       " '–',\n",
       " '—',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '…',\n",
       " '┾'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_messages_str = ''\n",
    "\n",
    "for index, value in enumerate(training_set.SMS):\n",
    "    all_messages_str = ' '.join([all_messages_str, value])\n",
    "\n",
    "set(re.findall('\\W', all_messages_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table immediately below gives a short resume of various attempts of improving the algorithm's accuracy by tweaking what words/characters should be recognized in a message. The differences between versions 'a' and 'b' is that in the former, the message is converted into lower case, so that the algorithm recognizes the same word regardless of letter case. Version 'b' maintains the original case set. Because each variation on the algorithm has its own set of new procedures, which need to be thoroughly justified in order to make sense, I made a support Jupyter notebook - 'p14 - Spam Filter - Alternative Algorithm Versions (II, III and IV)', which explains the thought process behind the changes to the original algorithm.  \n",
    "\n",
    "| Version | Case sensitive | Accuracy (%) | Description                                                 |\n",
    "|---------|-----------|--------------|-------------------------------------------------------------|\n",
    "| 1a      | no        | 98.74        | Original version.                                         |\n",
    "| 1b      | yes       | 98.47        | ---                                                         |\n",
    "| 2a      | no        | 98.83        | Recognizes currency symbols: '£', '€' and '$'.                |\n",
    "| 2b      | yes       | 98.56        | ---                                                         |\n",
    "| 3a      | no        | 98.83        | Same as V2 but recognizes references to GBP pences. |\n",
    "| 3b      | yes        | 98.74        | ---                                                         |\n",
    "| 4a      | no        | 98.65       | Separate all numbers from letters and from other characters/symbols.|\n",
    "| 4b      | yes       |   98.47       | ---                                                         |\n",
    "\n",
    "\n",
    "In more detail, version 2 allows the algorithm to recognize the aforementioned currency symbols as a word (delimited with whitespaces), maintaining everything else the same as the original version. Version 3 adds to the version 2 a trench of code that allows the algorithm to also recognize references to British pound pences, as mentioned above, e.g. a message that includes this expression - '150p/text', will have it converted to '150p text', so that the reference to 150 pences can be recognized. Version 4 allows every sequence of either letters, digits or symbols to be isolated by a whitespace, and thus recognized as a single expression, e.g.: '150p/text' would become '150 p / text'. The idea for this version is that spam messages may have unusual or an excessive inclusion of hyperbolic symbols/characters such as many exclamation points. \n",
    "\n",
    "Analyzing the table we can observe that the task of improving on the original accuracy test will be very difficult, given that it is already at 1.26\\% of being 100\\% accurate. The main point to be taken here is that tweaking the original algorithm did not improve or worsen significantly the algorithm's accuracy. Version's 2a and 3a were capable of accurately classify one more message correctly (each message is 0.09\\% from the total, which is the difference seen from version 1a to versions 2b and 3b). One detail that stands out is that making the the algorithm case sensitive slightly decreases its accuracy in all scenarios.   \n",
    "\n",
    "Since these variations of the algorithm were not up to the difficult task of reaching the 100\\% accuracy, other implementations can be tried in future versions. These could be based on the attribution of a certain weight to the probability of spam, P(Spam|w1, w2,..., wn), given the number of capitalized words in a message: spam messages tend to be more hyperbolic, using more capital words; or by exploring the relation between spam messages and the number of characters per message (in relation to non-spam messages), in order to try to predict whether a really long message, with many characters, has the same probability of being spam or non-spam, when there is a suspicion that spam messages may be shorter and more to the point, than other types of messages. \n",
    "\n",
    "As a last remark, we can at least recognize that there was room to improve from the original algorithm's version, and if the algorithm was tested in other testing sets, the idea of recognizing symbols and expressions related to currency when classifying a message as spam or not is relevant. \n",
    "\n",
    "\\[End of Project\\]\n",
    "\n",
    "\n",
    "\\***"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "179px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "600.85px",
    "left": "1545px",
    "right": "20px",
    "top": "120px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
