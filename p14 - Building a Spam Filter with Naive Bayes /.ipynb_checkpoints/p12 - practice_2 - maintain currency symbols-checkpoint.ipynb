{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Project 12: Building a Spam Filter with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 1\n",
    "---\n",
    "We've come a long way in this course — we've learned to:\n",
    "\n",
    "\n",
    "- Assign probabilities to events based on certain conditions by using conditional probability rules.\n",
    "\n",
    "\n",
    "- Assign probabilities to events based on whether they are in relationship of statistical independence or not with other events.\n",
    "\n",
    "\n",
    "- Assign probabilities to events based on prior knowledge by using Bayes' theorem.\n",
    "\n",
    "\n",
    "- Create a spam filter for SMS messages using the multinomial Naive Bayes algorithm.\n",
    "\n",
    "In our last lesson, we focused extensively on learning how the Naive Bayes algorithm works from a theoretical standpoint (more specifically, we learned about the multinomial Naive Bayes algorithm). In this guided project, we're going to study the practical side of the algorithm by building a spam filter for SMS messages.\n",
    "\n",
    "To classify messages as spam or non-spam, we saw in the previous lesson that the computer:\n",
    "\n",
    "\n",
    "1. Learns how humans classify messages.\n",
    "\n",
    "\n",
    "2. Uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam.\n",
    "\n",
    "\n",
    "3. Classifies a new message based on these probability values — if the probability for spam is greater, then it classifies the message as spam. Otherwise, it classifies it as non-spam (if the two probability values are equal, then we may need a human to classify the message).\n",
    "\n",
    "So our first task is to \"teach\" the computer how to classify messages. To do that, we'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans.\n",
    "\n",
    "The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the [The UCI Machine Learning](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection) Repository.\n",
    "\n",
    "Let's start by reading in the dataset. You'll be able to find the [solutions to this project at this link](https://github.com/dataquestio/solutions/blob/master/Mission433Solutions.ipynb) or by clicking the key icon at the top right of the interface.\n",
    "\n",
    "Note that due to the nature of spam messages, the dataset contains content that may be offensive to some users.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. To help readers gain context into your project, use the first Markdown cell of the notebook to add a title and a short introduction where you concisely explain what the project is about and what your goal is in this project (the title and the introduction are tentative at this point, so don't spend too much time here — you can come back at the end of your work to refine them).\n",
    "\n",
    "\n",
    "2. Open the `SMSSpamCollection` file using the `read_csv()` function from the pandas package.\n",
    "    - The data points are tab separated, so we'll need to use the `sep='\\t'` parameter for our `read_csv()` function.\n",
    "    - The dataset doesn't have a header row, which means we need to use the `header=None` parameter, otherwise the first row will be wrongly used as the header row.\n",
    "    - Use the `names=['Label', 'SMS']` parameter to name the columns as `Label` and `SMS`.\n",
    "\n",
    "\n",
    "3. Explore the dataset a little.\n",
    "    - Find how many rows and columns it has.\n",
    "    - Find what percentage of the messages is spam and what percentage is ham (\"ham\" means non-spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project aims at creating a spam filter that resorts to a multinomial Naive Bayes algorithm in order to distinguish spam messages from regular ones. The filter will be tested on a data set comprised of 5572 messages that have been previously determined by humans, if they are spam or not.\n",
    "\n",
    "The spam filter is considered to be successful if it can filter out 80% of the spam from a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spam data set to be worked with will be named `sms_spam`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_spam_full = pd.read_csv('SMSSpamCollection.txt',\n",
    "                   sep='\\t',\n",
    "                   names=['Label', 'SMS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Label   5572 non-null   object\n",
      " 1   SMS     5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 43.6+ KB\n"
     ]
    }
   ],
   "source": [
    "sms_spam_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Label` column only has two values:\n",
    "\n",
    "- `spam`.\n",
    "- `ham` (not spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label                     SMS\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_spam_full.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can see that only aprox. 13.4% of the messages are spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.6\n",
       "spam    13.4\n",
       "Name: ham vs spam (%), dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_label = sms_spam_full.Label.value_counts(normalize=True).round(3)*100\n",
    "\n",
    "count_label = count_label.rename('ham vs spam (%)')\n",
    "\n",
    "count_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 2\n",
    "---\n",
    "On the previous screen, we read in the dataset and saw that about 87% of the messages are ham (\"ham\" means non-spam), and the remaining 13% are spam. Now that we've become a bit familiar with the dataset, we can move on to building the spam filter.\n",
    "\n",
    "However, before creating it, it's very helpful to first think of a way of testing how well it works. When creating software (a spam filter is software), a good rule of thumb is that designing the test comes before creating the software. If we write the software first, then it's tempting to come up with a biased test just to make sure the software passes it.\n",
    "\n",
    "\n",
    "Once our spam filter is done, we'll need to test how good it is with classifying new messages. To test the spam filter, we're first going to split our dataset into two categories:\n",
    "\n",
    "\n",
    "- A **training set**, which we'll use to \"train\" the computer how to classify messages.\n",
    "\n",
    "\n",
    "- A **test set**, which we'll use to test how good the spam filter is with classifying new messages.\n",
    "\n",
    "\n",
    "We're going to keep 80% of our dataset for training, and 20% for testing (we want to train the algorithm on as much data as possible, but we also want to have enough test data). The dataset has 5,572 messages, which means that:\n",
    "\n",
    "\n",
    "- The training set will have 4,458 messages (about 80% of the dataset).\n",
    "\n",
    "\n",
    "- The test set will have 1,114 messages (about 20% of the dataset).\n",
    "\n",
    "To better understand the purpose of putting a test set aside, let's begin by observing that all 1,114 messages in our test set are already classified by a human. When the spam filter is ready, we're going to treat these messages as new and have the filter classify them. Once we have the results, we'll be able to compare the algorithm classification with that done by a human, and this way we'll see how good the spam filter really is.\n",
    "\n",
    "**For this project, our goal is to create a spam filter that classifies new messages with an accuracy greater than 80%** — so we expect that more than 80% of the new messages will be classified correctly as spam or ham (non-spam).\n",
    "\n",
    "We'll come back to testing toward the end of this guided project, but for now, let's create a training and a test set. We're going to start by randomizing the entire dataset to ensure that spam and ham messages are spread properly throughout the dataset. \n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. Start by randomizing the entire dataset by using the `DataFrame.sample()` method.\n",
    "    - Use the `frac=1` parameter to randomize the entire dataset.\n",
    "    - Use the `random_state=1` parameter to make sure your results are reproducible.\n",
    "\n",
    "\n",
    "2. Split the randomized dataset into a training and a test set.\n",
    "    - The training set should account for 80% of the dataset, and the remaining 20% of the data should be the test set.\n",
    "    - Reset the index labels for both data sets — the index labels remained unordered after randomization. You can use the `DataFrame.reset_index()` method.\n",
    "\n",
    "\n",
    "3. Find the percentage of spam and ham in both the training and the test set. Are the percentages similar to what we have in the full dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a training set and a test set out of `sms_spam_full`:\n",
    "\n",
    "1. Randomizing the `sms_spam_full`.\n",
    "\n",
    "\n",
    "2. Split randomized DF into a training set (20%) and a testing set (remaining 80%).\n",
    "\n",
    "\n",
    "3. Compare the 'Label' value distribution of the entire randomized DF with the previously made subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "random_sms_spam = sms_spam_full.sample(n=None, frac=1, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From earlier on we know that `random_sms_spam` has 5572 entries (0 to 5571)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of observations/rows in random_sms_spam: 5572 \n",
      "20% of total observations/rows in random_sms_spam: 4458.0\n"
     ]
    }
   ],
   "source": [
    "eighty_perc = random_sms_spam.shape[0] * 0.8\n",
    "\n",
    "print(f'Total number of observations/rows in random_sms_spam: {random_sms_spam.shape[0]}', \n",
    "      f'\\n20% of total observations/rows in random_sms_spam: {round(eighty_perc, 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the information above we set the training set as the rows 0 to 4458, 80% of the total rows in `random_sms_spam`. The remaining rows, will form the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "training_set = random_sms_spam.copy().iloc[:4458+1, :]\n",
    "\n",
    "testing_set = random_sms_spam.copy().iloc[4458:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, comparing the distribution of values in the `Label` column across DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_sms_spam</th>\n",
       "      <th>count_label_training</th>\n",
       "      <th>count_label_testing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>86.6</td>\n",
       "      <td>86.5</td>\n",
       "      <td>86.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>13.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      random_sms_spam  count_label_training  count_label_testing\n",
       "ham              86.6                  86.5                 86.8\n",
       "spam             13.4                  13.5                 13.2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.\n",
    "\n",
    "# Training set.\n",
    "count_label_training = training_set.Label.value_counts(normalize=True).round(3)*100\n",
    "\n",
    "count_label_training = count_label_training.rename('ham vs spam (%)')\n",
    "\n",
    "# Testing set.\n",
    "count_label_testing = testing_set.Label.value_counts(normalize=True).round(3)*100\n",
    "\n",
    "count_label_testing = count_label_testing.rename('ham vs spam (%)')\n",
    "\n",
    "\n",
    "# Combining all label percentage counts for comparison.\n",
    "compare_label = pd.DataFrame({'random_sms_spam': count_label,\n",
    "                              'count_label_training': count_label_training,\n",
    "                              'count_label_testing': count_label_testing})\n",
    "\n",
    "compare_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the table above, the value distribution is very similar across the panel, meaning that we can infer the conclusions produced from the training set to the testing set, since both sets resemble the original series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving RAM 1\n",
    "del sms_spam_full\n",
    "del random_sms_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 3\n",
    "---\n",
    "On the previous screen, we split our dataset into a training set and a test set. The next big step is to use the training set to teach the algorithm to classify new messages.\n",
    "\n",
    "Recall from the previous lesson that when a new message comes in, our Naive Bayes algorithm will make the classification based on the results it gets to these two equations (note that we replaced \"$Spam^C$\" with \"$Ham$\" inside the second equation below):\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "Also, to calculate $P(w_i|Spam)$ and $P(w_i|Ham)$ inside the formulas above, recall that we need to use these equations:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "Let's also summarize what the terms in the equations above mean:\n",
    "\n",
    "\\begin{align}\n",
    "&N_{w_i|Spam} = \\text{the number of times the word } w_i \\text{ occurs in spam messages} \\\\\n",
    "&N_{w_i|Spam^C} = \\text{the number of times the word } w_i \\text{ occurs in non-spam messages} \\\\\n",
    "\\\\\n",
    "&N_{Spam} = \\text{total number of words in spam messages} \\\\\n",
    "&N_{Spam^C} = \\text{total number of words in non-spam messages} \\\\\n",
    "\\\\\n",
    "&N_{Vocabulary} = \\text{total number of words in the vocabulary} \\\\\n",
    "&\\alpha = 1 \\ \\ \\ \\ (\\alpha \\text{ is a smoothing parameter})\n",
    "\\end{align}\n",
    "\n",
    "To calculate all these probabilities, we'll first need to perform a bit of data cleaning to bring the data in a format that will allow us to extract easily all the information we need. Right now, our training and test sets have this format (the messages are fictitious to make the example easier to understand):\n",
    "\n",
    "![img_1](1.jpg)\n",
    "\n",
    "\n",
    "To make the calculations easier, we want bring the data to this format (the table below is a transformation of the table you see above):\n",
    "\n",
    "\n",
    "![img_2](2.jpg)\n",
    "\n",
    "\n",
    "About the transformation above, notice that:\n",
    "\n",
    "- The `SMS` column doesn't exist anymore.\n",
    "\n",
    "\n",
    "- Instead, the `SMS` column is replaced by a series of new columns, where each column represents a unique word from the vocabulary.\n",
    "\n",
    "\n",
    "- Each row describes a single message. For instance, the first row corresponds to the message \"SECRET PRIZE! CLAIM SECRET PRIZE NOW!!\", and it has the values `spam, 2, 2, 1, 1, 0, 0, 0, 0, 0`. These values tell us that:\n",
    "    - The message is spam.\n",
    "    - The word \"secret\" occurs two times inside the message.\n",
    "    - The word \"prize\" occurs two times inside the message.\n",
    "    - The word \"claim\" occurs one time inside the message.\n",
    "    - The word \"now\" occurs one time inside the message.\n",
    "    - The words \"coming\", \"to\", \"my\", \"party\", and \"winner\" occur zero times inside the message.\n",
    "    \n",
    "    \n",
    "- All words in the vocabulary are in lower case, so \"SECRET\" and \"secret\" come to be considered to be the same word.\n",
    "\n",
    "\n",
    "- Punctuation is not taken into account anymore (for instance, we can't look at the table and conclude that the first message initially had three exclamation marks).\n",
    "\n",
    "Let's begin the data cleaning process by removing the punctuation and bringing all the words to lower case.\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. Remove all the punctuation from the `SMS` column. You can use the regex `'\\W'` to detect any character that is not from a-z, A-Z or 0-9.\n",
    "    - For instance, the function `re.sub('\\W', ' ', 'Secret!! Money, goods.' )` strips the punctuation marks and outputs the string 'Secret Money goods '.\n",
    "    - For simplicity, you can use the `Series.str.replace()` method.\n",
    "    \n",
    "    \n",
    "2. For each message, transform every letter in every word to lower case. You may want to use the `Series.str.lower()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next stage we focus on working with the training set. In order to apply the Naive Bayes Theorem, we can build a table that, for each message, classifies it as spam or not spam and counts the number of times a word, from the vocabulary, appears in the message. Whenever a message does not contain a word from the vocabulary, each word in the vocabulary having its own column, it registers `0`. Remember that the vocabulary is the group of all unique words gathered from all the messages within the training set.\n",
    "\n",
    "Prior to building this DataFrame, two cleaning steps applied to the `SMS` column will be undertaken:\n",
    "\n",
    "- eliminate punctuation.\n",
    "- lower case every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_messages_str = ''\n",
    "\n",
    "for index, value in enumerate(training_set.SMS):\n",
    "    all_messages_str = ' '.join([all_messages_str, value])\n",
    "    \n",
    "\n",
    "non_words_set = set(re.findall('\\W', all_messages_str))\n",
    "\n",
    "\n",
    "non_words_list = list(non_words_set)\n",
    "\n",
    "non_words_list_w_spaces = [' ' + i + ' ' for i in non_words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['»',\n",
       " '\"',\n",
       " '[',\n",
       " '(',\n",
       " '\\x91',\n",
       " '<',\n",
       " '\\x93',\n",
       " '=',\n",
       " '¡',\n",
       " '.',\n",
       " ',',\n",
       " '“',\n",
       " '^',\n",
       " '┾',\n",
       " '&',\n",
       " '\\x92',\n",
       " ' ',\n",
       " '?',\n",
       " '|',\n",
       " ')',\n",
       " '~',\n",
       " '-',\n",
       " '\\n',\n",
       " '\\x96',\n",
       " '%',\n",
       " '@',\n",
       " ':',\n",
       " '…',\n",
       " \"'\",\n",
       " '*',\n",
       " '\\\\',\n",
       " '$',\n",
       " '’',\n",
       " '>',\n",
       " ']',\n",
       " '!',\n",
       " ';',\n",
       " '–',\n",
       " '+',\n",
       " '\\x94',\n",
       " '\\t',\n",
       " '/',\n",
       " '—',\n",
       " '#',\n",
       " '£',\n",
       " '‘']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           Yep by the pretty sculpture\n",
       "1           Yes princess Are you going to make me moan \n",
       "2                            Welp apparently he retired\n",
       "3                                               Havent \n",
       "4     I forgot 2 ask all smth There s a card on da p...\n",
       "5     Ok i thk i got it Then u wan me 2 come now or ...\n",
       "6     I want kfc its Tuesday Only buy 2 meals ONLY 2...\n",
       "7                              No dear i was sleeping P\n",
       "8                                Ok pa Nothing problem \n",
       "9                             Ill be there on lt gt ok \n",
       "10    My uncles in Atlanta Wish you guys a great sem...\n",
       "11                                             My phone\n",
       "12                         Ok which your another number\n",
       "13    The greatest test of courage on earth is to be...\n",
       "14    Dai what this da Can i send my resume to this id \n",
       "15                         I am late I will be there at\n",
       "16    FreeMsg Why haven t you replied to my text I m...\n",
       "17                     K text me when you re on the way\n",
       "18    Congrats 2 mobile 3G Videophones R yours call ...\n",
       "19      Please leave this topic sorry for telling that \n",
       "20    Ooooooh I forgot to tell u I can get on yovill...\n",
       "21             Hi this is yijue can i meet u at 11 tmr \n",
       "22    I want to show you the world princess how abou...\n",
       "23                    Well that must be a pain to catch\n",
       "24                    Well You know what i mean Texting\n",
       "25          Your bill at 3 is £ 33 65 so thats not bad \n",
       "26                          Yeah where s your class at \n",
       "27                                       What s ur pin \n",
       "28    Fighting with the world is easy u either win o...\n",
       "29    Dude What s up How Teresa Hope you have been o...\n",
       "Name: SMS, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training set `SMS` cleaned series\n",
    "\n",
    "ts_cleaned_SMS = training_set.SMS.copy().str.replace('[^A-Za-z0-9\\s£€\\$]', ' ', regex=True)\n",
    "ts_cleaned_SMS = ts_cleaned_SMS.str.replace('€', ' € ', regex=False)\n",
    "ts_cleaned_SMS = ts_cleaned_SMS.str.replace('£', ' £ ', regex=True)\n",
    "ts_cleaned_SMS = ts_cleaned_SMS.str.replace('\\$', ' $ ', regex=True)\n",
    " \n",
    "#`\\s+` ensures that if there are two or more joined whitespaces they are converted to just one.\n",
    "ts_cleaned_SMS = ts_cleaned_SMS.str.replace('\\s+', ' ', regex=True) \n",
    "\n",
    "ts_cleaned_SMS.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last version of `ts_cleaned_SMS` still has rows which have whitespaces at the beginning or end of the message which can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_cleaned_SMS = ts_cleaned_SMS.str.replace('(\\A +| +\\Z)', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we confirm that there is no whitespaces at the beginning or end of the row/string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: SMS, dtype: bool)\n",
      "Series([], Name: SMS, dtype: bool)\n"
     ]
    }
   ],
   "source": [
    "cond1 = ts_cleaned_SMS.str.contains(pat='(?:\\A +| +\\Z)')\n",
    "\n",
    "ts_cleaned_SMS[cond1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case for every string.\n",
    "ts_cleaned_SMS = ts_cleaned_SMS.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the changes in `training_set.SMS` were successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          yep by the pretty sculpture\n",
       "1           yes princess are you going to make me moan\n",
       "2                           welp apparently he retired\n",
       "3                                               havent\n",
       "4    i forgot 2 ask all smth there s a card on da p...\n",
       "Name: SMS, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_cleaned_SMS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 4\n",
    "---\n",
    "On the previous screen, we removed the punctuation and changed all letters to lowercase. Recall that our end goal with this data cleaning process is to bring our training set to this format:\n",
    "\n",
    "![img_3](3.jpg)\n",
    "\n",
    "With the exception of the \"Label\" column, every other column in the transformed table above represents a unique word in our vocabulary (more specifically, each column shows the frequency of that unique word for any given message). Recall from the previous lesson that we call the set of unique words a **vocabulary**.\n",
    "\n",
    "We'll eventually bring the training set to that format ourselves, but first, let's create a list with all of the unique words that occur in the messages of our training set.\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. Create a vocabulary for the messages in the training set. The vocabulary should be a Python list containing all the unique words across all messages, where each word is represented as a string.\n",
    "\n",
    "- Begin by transforming each message from the `SMS` column into a list by splitting the string at the space character — use the `Series.str.split()` method.\n",
    "\n",
    "\n",
    "- Initiate an empty list named `vocabulary`.\n",
    "\n",
    "\n",
    "- Iterate over the the `SMS` column (each message in this column should be a list of strings by the time you start this loop).\n",
    "    - Using a nested loop, iterate each message in the `SMS` column (each message should be a list of strings) and append each string (word) to the vocabulary list.\n",
    "    \n",
    "    \n",
    "- Transform the `vocabulary` list into a set using the `set()` function. This will remove the duplicates from the `vocabulary` list.\n",
    "\n",
    "\n",
    "- Transform the `vocabulary` set back into a list using the `list()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next stage is producing the vocabulary (set of unique words), with the following steps:\n",
    "\n",
    "1. Split messages into columns - one unique string per column (removing most of the whitespaces as well).\n",
    "2. Concatenating all the columns of strings into a single Series.\n",
    "3. Dropping Nan-values.\n",
    "3. Convert Series into a list.\n",
    "4. Converting list into a set, thus excluding duplicated values/words.\n",
    "5. Convert set into back into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "ts_cleaned_SMS_split = ts_cleaned_SMS.str.split(' ', expand=True) \n",
    "\n",
    "# 2.\n",
    "ts_cleaned_SMS_split_cat = pd.concat([ts_cleaned_SMS_split.iloc[i, :] for i in range(0, ts_cleaned_SMS_split.shape[0])],\n",
    "                                     ignore_index=True)\n",
    "# 3.\n",
    "ts_cleaned_SMS_split_cat = ts_cleaned_SMS_split_cat.dropna()\n",
    "\n",
    "# 4.\n",
    "ts_cleaned_SMS_split_cat_to_list = ts_cleaned_SMS_split_cat.to_list()\n",
    "\n",
    "#5.\n",
    "vocabulary_set = set(ts_cleaned_SMS_split_cat_to_list)\n",
    "\n",
    "#6.\n",
    "vocabulary = list(vocabulary_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When checking `vocabulary` below, we see that whitespace still appears as a value, thus it can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'cookies', 'uncles', 'shaping', 'shoranur']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cookies', 'uncles', 'shaping', 'shoranur', 'frank']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, el in enumerate(vocabulary):\n",
    "    if el == '':\n",
    "        del vocabulary[index]\n",
    "        \n",
    "vocabulary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free RAM 2\n",
    "\n",
    "del ts_cleaned_SMS_split_cat \n",
    "del ts_cleaned_SMS_split_cat_to_list\n",
    "del vocabulary_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 5\n",
    "---\n",
    "On the previous screen, we managed to create the vocabulary for our messages in the training set. Now we're going to use the vocabulary to make the data transformation we need:\n",
    "\n",
    "![img_4](4.jpg)\n",
    "\n",
    "Eventually, we're going to create a new DataFrame. However, we'll first build a dictionary that we'll then convert to the DataFrame we need.\n",
    "\n",
    "For instance, to create the table we see above, we could use this dictionary and then convert it to a DataFrame:\n",
    "\n",
    "    word_counts_per_sms = {'secret': [2,1,1],\n",
    "                           'prize': [2,0,1],\n",
    "                           'claim': [1,0,1],\n",
    "                           'now': [1,0,1],\n",
    "                           'coming': [0,1,0],\n",
    "                           'to': [0,1,0],\n",
    "                           'my': [0,1,0],\n",
    "                           'party': [0,1,0],\n",
    "                           'winner': [0,0,1]\n",
    "                          }\n",
    "\n",
    "\n",
    "    word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "    word_counts.head()\n",
    "\n",
    "Output:\n",
    "\n",
    "|   |secret|prize|claim|now|coming|to |my |party|winner|\n",
    "|---|------|-----|-----|---|------|---|---|-----|------|\n",
    "|0  |2     |2    |1    |1  |0     |0  |0  |0    |0     |\n",
    "|1  |1     |0    |0    |0  |1     |1  |1  |1    |0     |\n",
    "|2  |1     |1    |1    |1  |0     |0  |0  |0    |1     |\n",
    "\n",
    "(As you may have noticed from the output above, the `Label` column is missing, but we'll get to that in the next exercise.)\n",
    "\n",
    "To create the dictionary we need for our training set, we can use the code below, where:\n",
    "\n",
    "- We start by initializing a dictionary named `word_counts_per_sms`, where each key is a unique word (a string) from the vocabulary, and each value is a list of the length of training set, where each element in the list is a `0`.\n",
    "    - The code `[0] * 5` outputs `[0, 0, 0, 0, 0]`. So the code `[0] * len(training_set['SMS']`) outputs a list of the length of `training_set['SMS']`, where each element in the list will be a `0`.\n",
    "\n",
    "\n",
    "- We loop over `training_set['SMS']` using at the same time the `enumerate()` function to get both the `index` and the SMS message (index and `sms`).\n",
    "    - Using a nested loop, we loop over `sms `(where `sms` is a list of strings, where each string represents a word in a message).\n",
    "        - We increment `word_counts_per_sms[word][index]` by `1`.\n",
    "\n",
    "---\n",
    "\n",
    "    word_counts_per_sms = {unique_word: [0] * \n",
    "        len(training_set['SMS']) for unique_word in vocabulary}\n",
    "\n",
    "    for index, sms in enumerate(training_set['SMS']):\n",
    "        for word in sms:\n",
    "            word_counts_per_sms[word][index] += 1\n",
    "\n",
    "Now that we have the dictionary we need, let's do the final transformations to our training set and then move forward with creating the spam filter.\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. Run the code you see above to get the `word_counts_per_sms` dictionary. In case you want to do a bit of exploration, note that this is a large dictionary, and printing it all is not recommended (you should rather use a for loop and print only the first five or so key-value pairs).\n",
    "\n",
    "\n",
    "2. Transform `word_counts_per_sms` into a DataFrame using `pd.DataFrame()`.\n",
    "\n",
    "\n",
    "3. Concatenate the DataFrame we just built above with the DataFrame containing the training set (this way, we'll also have the `Label` and the` SMS` columns). Use the `pd.concat()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to what is in the instructions we create a splitted version of `ts_cleaned_SMS` that does not expand each string into a single column but compiles the message into a list with strings.\n",
    "\n",
    "Similarly to `vocabulary`, I delete empty strings, `''`; this time resorting to a custom function - `remove_elements` that is applied to `ts_cleaned_SMS_split_listed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_elements(list_x, list_strings):\n",
    "    \"\"\"Strings in list_strings are removed from list_x if this later \n",
    "    list contains any of those strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    for index, el in enumerate(list_x):\n",
    "        if el in list_strings:\n",
    "            del list_x[index]\n",
    "    \n",
    "    return list_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_cleaned_SMS_split_listed = ts_cleaned_SMS.str.split(' ') # `expand=False` by default\n",
    "\n",
    "strings_to_remove = ['']\n",
    "\n",
    "ts_cleaned_SMS_split_listed_1 = ts_cleaned_SMS_split_listed.copy().apply(lambda x: remove_elements(x, strings_to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested by the tutorial, first a dictionary is created and then I go through every message and count each word repetition (each word is a key), filling out the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the tutorial.\n",
    "\n",
    "# 1.\n",
    "word_counts_per_sms = {unique_word: [0] * len(ts_cleaned_SMS_split_listed_1) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(ts_cleaned_SMS_split_listed_1):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert dictionary into DataFrame\n",
    "\n",
    "word_counts_per_sms_df = pd.DataFrame(word_counts_per_sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing if the conversion was successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postal</th>\n",
       "      <th>pleassssssseeeeee</th>\n",
       "      <th>titles</th>\n",
       "      <th>couple</th>\n",
       "      <th>deficient</th>\n",
       "      <th>vegetables</th>\n",
       "      <th>settings</th>\n",
       "      <th>finding</th>\n",
       "      <th>aiya</th>\n",
       "      <th>walkin</th>\n",
       "      <th>...</th>\n",
       "      <th>missin</th>\n",
       "      <th>max6</th>\n",
       "      <th>quiet</th>\n",
       "      <th>bishan</th>\n",
       "      <th>comedy</th>\n",
       "      <th>sterling</th>\n",
       "      <th>braved</th>\n",
       "      <th>token</th>\n",
       "      <th>into</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 7770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   postal  pleassssssseeeeee  titles  couple  deficient  vegetables  settings  \\\n",
       "0       0                  0       0       0          0           0         0   \n",
       "1       0                  0       0       0          0           0         0   \n",
       "\n",
       "   finding  aiya  walkin  ...  missin  max6  quiet  bishan  comedy  sterling  \\\n",
       "0        0     0       0  ...       0     0      0       0       0         0   \n",
       "1        0     0       0  ...       0     0      0       0       0         0   \n",
       "\n",
       "   braved  token  into  weight  \n",
       "0       0      0     0       0  \n",
       "1       0      0     0       0  \n",
       "\n",
       "[2 rows x 7770 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_per_sms_df.iloc[:2, 10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, concatenating `training_set` with `word_counts_per_sms_df` into a new DataFrame - `training_set_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. \n",
    "\n",
    "# `sort=False` is required to preserve the order of the columns in a 'first in' fashion.\n",
    "training_set_2 = pd.concat([training_set, word_counts_per_sms_df], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>cookies</th>\n",
       "      <th>uncles</th>\n",
       "      <th>shaping</th>\n",
       "      <th>shoranur</th>\n",
       "      <th>frank</th>\n",
       "      <th>icmb3cktz8r7</th>\n",
       "      <th>150pm</th>\n",
       "      <th>winning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yep, by the pretty sculpture</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes, princess. Are you going to make me moan?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                            SMS  cookies  uncles  \\\n",
       "0   ham                   Yep, by the pretty sculpture        0       0   \n",
       "1   ham  Yes, princess. Are you going to make me moan?        0       0   \n",
       "\n",
       "   shaping  shoranur  frank  icmb3cktz8r7  150pm  winning  \n",
       "0        0         0      0             0      0        0  \n",
       "1        0         0      0             0      0        0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_2.iloc[:2, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing whether the rows in `training_set` were well aligned with the correspondent rows in `word_counts_per_sms_df`, two examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: row 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set_2[\"SMS\"], row 0: Yep, by the pretty sculpture\n",
      "\n",
      "\n",
      "training_set_2, row 0 - columns that are \"1 or more\":\n",
      "\n",
      "by           1\n",
      "the          1\n",
      "pretty       1\n",
      "sculpture    1\n",
      "yep          1\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cond_row_0 = training_set_2.iloc[0, 2:] == 1 \n",
    "\n",
    "print('training_set_2[\"SMS\"], row 0: {}'.format(training_set_2.iloc[0, 1]))\n",
    "print('\\n')\n",
    "print('training_set_2, row 0 - columns that are \"1 or more\":\\n\\n{}'.format(training_set_2.iloc[0, 2:][cond_row_0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: row 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, princess. Are you going to make me moan?\n",
      "\n",
      "\n",
      "princess    1\n",
      "going       1\n",
      "make        1\n",
      "you         1\n",
      "to          1\n",
      "yes         1\n",
      "are         1\n",
      "moan        1\n",
      "me          1\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cond_row_1 = training_set_2.iloc[1, 2:] == 1 \n",
    "\n",
    "print(training_set_2.iloc[1, 1])\n",
    "print('\\n')\n",
    "print(training_set_2.iloc[1, 2:][cond_row_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free RAM 3\n",
    "\n",
    "del ts_cleaned_SMS_split_listed\n",
    "del word_counts_per_sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 6\n",
    "---\n",
    "Now that we're done with data cleaning and have a training set to work with, we can begin creating the spam filter. Recall that the Naive Bayes algorithm will need to know the probability values of the two equations below to be able to classify new messages:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam) \\\\\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "Also, to calculate $P(w_i|Spam)$ and $P(w_i|Ham)$ inside the formulas above, recall that we need to use these equations:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}} \\\\\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Some of the terms in the four equations above will have the same value for every new message. As a start, let's first calculate:\n",
    "\n",
    "- $P(Spam)$ and $P(Ham)$.\n",
    "\n",
    "\n",
    "- $N_{Spam}$, $N_{Ham}$, $N_{Vocabulary}$.\n",
    "\n",
    "\n",
    "Recall from the previous lesson that:\n",
    "\n",
    "- $N_{Spam}$ is equal to the number of words in all the spam messages — it's not equal to the number of spam messages, and it's _not_ equal to the total number of _unique_ words in spam messages.\n",
    "\n",
    "\n",
    "- $N_{Ham}$ is equal to the number of words in all the non-spam messages — it's _not_ equal to the number of non-spam messages, and it's not equal to the total number of _unique_ words in non-spam messages.\n",
    "\n",
    "- We'll also use Laplace smoothing and set $α=1$.\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. Calculate P(Spam) and P(Ham). There's more than one way to write the code that can calculate this — feel free to choose any solution you want.\n",
    "\n",
    "\n",
    "2. Calculate $N_{Spam}$, $N_{Ham}$, $N_{Vocabulary}$. Feel free to choose any programming solution you like.\n",
    "\n",
    "\n",
    "3. Initiate a variable named alpha with a value of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the elements within the Naive Bayes algorithm. Starting with:\n",
    "\n",
    "- $P(Spam)$ and $P(Ham)$.\n",
    "- $N_{Spam}$, $N_{Ham}$, $N_{Vocabulary}$\n",
    "\n",
    "The Laplace smoothing parameter is set to 1: $α=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of Spam and Ham. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts_ts2 = training_set_2.Label.value_counts(normalize=True)\n",
    "\n",
    "p_spam = label_counts_ts2.spam\n",
    "\n",
    "p_ham = label_counts_ts2.ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13455931823278763"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8654406817672123"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting $N_{Spam}$, $N_{Ham}$, $N_{Vocabulary}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7780"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_vocabulary.\n",
    "\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "n_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up the total number of words for spam messages and for non-spam messages - $N_{Spam}$, $N_{Ham}$ respectively, the procedure will be the following:\n",
    "\n",
    "1. add a column summing up the number of words per message/row.\n",
    "\n",
    "2. calculate sum of words if row if messages are spam; same for non-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "training_set_2['sum_words_sms'] = training_set_2.iloc[:, 2:].copy().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "\n",
    "n_spam = training_set_2.loc[training_set_2.Label=='spam', 'sum_words_sms'].sum()\n",
    "\n",
    "n_ham = training_set_2.loc[training_set_2.Label=='ham', 'sum_words_sms'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_spam: 15456 \n",
      "n_ham: 57129\n"
     ]
    }
   ],
   "source": [
    "print(f'n_spam: {n_spam}',\n",
    "     f'\\nn_ham: {n_ham}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally setting $α=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 7\n",
    "---\n",
    "On the previous screen, we managed to calculate a few terms for our equations:\n",
    "\n",
    "- $P(Spam)$ and $P(Ham)$.\n",
    "\n",
    "\n",
    "- $N_{Spam}$, $N_{Ham}$, $N_{Vocabulary}$.\n",
    "\n",
    "\n",
    "As we've already mentioned, all these terms will have constant values in our equations for every new message (regardless of the message or each individual word in the message).\n",
    "\n",
    "However, $P(w_i|Spam)$ and $P(w_i|Ham)$ will vary depending on the individual words. For instance, P(\"secret\"|Spam) will have a certain probability value, while P(\"cousin\"|Spam) or P(\"lovely\"|Spam) will most likely have other values.\n",
    "\n",
    "Although both $P(w_i|Spam)$ and $P(w_i|Ham)$ vary depending on the word, the probability for each individual word is constant for every new message.\n",
    "\n",
    "\n",
    "\n",
    "For instance, let's say we receive two new messages:\n",
    "\n",
    "\n",
    "- \"secret code\".\n",
    "\n",
    "\n",
    "- \"secret party 2night\".\n",
    "\n",
    "\n",
    "We'll need to calculate P(\"secret\"|Spam) for both these messages, and we can use the training set to get the values we need to find a result for the equation below:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\text{\"secret\"}|Spam) = \\frac{N_{\"secret\"|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "The steps we take to calculate P(\"secret\"|Spam) will be identical for both of our new messages above, or for any other new message that contains the word \"secret\". The key detail here is that calculating P(\"secret\"|Spam) only depends on the training set, and as long as we don't make changes to the training set, P(\"secret\"|Spam) stays constant. The same reasoning also applies to P(\"secret\"|Ham).\n",
    "\n",
    "This means that we can use our training set to calculate the probability for each word in our vocabulary. If our vocabulary contained only the words \"lost\", \"navigate\", and \"sea\", then we'd need to calculate six probabilities:\n",
    "\n",
    "\n",
    "- P(\"lost\"|Spam) and P(\"lost\"|Ham)\n",
    "\n",
    "\n",
    "- P(\"navigate\"|Spam) and P(\"navigate\"|Ham)\n",
    "\n",
    "\n",
    "- P(\"sea\"|Spam) and P(\"sea\"|Ham)\n",
    "\n",
    "\n",
    "We have 7,783 words in our vocabulary, which means we'll need to calculate a total of 15,566 probabilities. For each word, we need to calculate both $P(w_i|Spam)$ and $P(w_i|Ham)$.\n",
    "\n",
    "In more technical language, the probability values that $P(w_i|Spam)$ and $P(w_i|Ham)$ will take are called **parameters**.\n",
    "\n",
    "The fact that we calculate so many values before even beginning the classification of new messages makes the Naive Bayes algorithm very fast (especially compared to other algorithms). When a new message comes in, most of the needed computations are already done, which enables the algorithm to almost instantly classify the new message.\n",
    "\n",
    "If we didn't calculate all these values beforehand, then all these calculations would need to be done every time a new message comes in. Imagine the algorithm will be used to classify 1,000,000 new messages. Why repeat all these calculations 1,000,000 times when we could just do them once at the beginning?\n",
    "\n",
    "Let's now calculate all the parameters using the equations below:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}} = \\frac{\\text{(number of times $w_i$ appears in spam messages)} + 1}{\\text{(total number of spam words)} + (1 * \\text{total number of unique words})} \\\\\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}} = \\frac{\\text{(number of times $w_i$ appears in ham messages)} + 1}{\\text{(total number of ham words)} + (1 * \\text{total number of unique words})} \\\\\n",
    "\\end{equation}\n",
    "\n",
    "Recall that $P(w_i|Spam)$ and $P(w_i|Ham)$ are key parts of the equations that we need to classify the new messages:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam) \\\\\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "\n",
    "\n",
    "1. Initialize two dictionaries, where each key-value pair is a unique word (from our vocabulary) represented as a string, and the value is `0`. We'll need one dictionary to store the parameters for $P(w_i|Spam)$, and the other for $P(w_i|Ham)$.\n",
    "    - If the entire vocabulary were `['sea', 'navigate']`, we'd need to initialize two dictionaries, one for spam and one for ham, and both should look like this: `{'sea': 0, 'navigate': 0}`.\n",
    "\n",
    "\n",
    "2. Isolate the spam and the ham messages in the training set into two different DataFrames. The `Label` column will help you isolate the messages.\n",
    "\n",
    "3. Iterate over the vocabulary, and, for each word, calculate $P(w_i|Spam)$ and $P(w_i|Ham)$ using the formulas we mentioned above.\n",
    "    - Recall that $N_{Spam}$, $N_{Ham}$, $N_{Vocabulary}$, and $α$ are already calculated from the last screen.\n",
    "    \n",
    "    - Recall from the previous lesson that $N_{w_i|Spam}$ is equal to the number of times the word $w_i$ occurs in all the spam messages, while $N_{w_i|Ham}$ is equal to the number of times the word $w_i$ occurs in all the ham messages.\n",
    "    \n",
    "- Once you're done with calculating an individual parameter, update the probability value in the two dictionaries you created initially.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To arrive to the dictionaries that contain ever $P(w_i|Spam)$ and $P(w_i|Ham)$, I will do the following:\n",
    "\n",
    "First, we create two Series that gives us $N_{w_i|Spam}$ and $N_{w_i|Ham}$:\n",
    "\n",
    "1. create two DataFrames that only contain either spam or non-spam messages.\n",
    "\n",
    "\n",
    "2. out of those two DataFrames create two correspondent Series - `sms_spam_sum` and `sms_ham_sum`; analogously for both Series, the row index is the column index of the former DataFrame, and each value of the Series corresponds to the sum of the values in each column. For a more intuitive understanding, see below the transformations made in `sms_spam`.\n",
    "\n",
    "\n",
    "3. the last step is to fill out two dictionaries, one for $P(w_i|Spam)$ and other for $P(w_i|Ham)$, based on the parameters parameters already calculated:\n",
    "    - `n_spam` and `n_ham`.\n",
    "    - `n_vocabulary`.\n",
    "    - $N_{w_i|Spam}$ and $N_{w_i|Ham}$.\n",
    "    - `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. the last colum 'sum_words_sms' is not included in the calculations\n",
    "# of these Series, so I set `.iloc[:, 2:-1]`.\n",
    "sms_spam = training_set_2.iloc[:, 2:-1].copy()[training_set_2.Label=='spam']\n",
    "\n",
    "sms_ham = training_set_2.iloc[:, 2:-1].copy()[training_set_2.Label=='ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cookies</th>\n",
       "      <th>uncles</th>\n",
       "      <th>shaping</th>\n",
       "      <th>shoranur</th>\n",
       "      <th>frank</th>\n",
       "      <th>icmb3cktz8r7</th>\n",
       "      <th>150pm</th>\n",
       "      <th>winning</th>\n",
       "      <th>heater</th>\n",
       "      <th>6669</th>\n",
       "      <th>...</th>\n",
       "      <th>missin</th>\n",
       "      <th>max6</th>\n",
       "      <th>quiet</th>\n",
       "      <th>bishan</th>\n",
       "      <th>comedy</th>\n",
       "      <th>sterling</th>\n",
       "      <th>braved</th>\n",
       "      <th>token</th>\n",
       "      <th>into</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 7780 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    cookies  uncles  shaping  shoranur  frank  icmb3cktz8r7  150pm  winning  \\\n",
       "16        0       0        0         0      0             0      0        0   \n",
       "18        0       0        0         0      0             0      1        0   \n",
       "56        0       0        0         0      0             0      0        0   \n",
       "\n",
       "    heater  6669  ...  missin  max6  quiet  bishan  comedy  sterling  braved  \\\n",
       "16       0     0  ...       0     0      0       0       0         0       0   \n",
       "18       0     0  ...       0     0      0       0       0         0       0   \n",
       "56       0     0  ...       0     0      0       0       0         0       0   \n",
       "\n",
       "    token  into  weight  \n",
       "16      0     0       0  \n",
       "18      0     0       0  \n",
       "56      0     0       0  \n",
       "\n",
       "[3 rows x 7780 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_spam.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "sms_spam_sum = sms_spam.sum().transpose()\n",
    "\n",
    "sms_ham_sum = sms_ham.sum().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cookies     0\n",
       "uncles      0\n",
       "shaping     0\n",
       "shoranur    0\n",
       "frank       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_spam_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "\n",
    "p_wi_given_spam_dict = {}\n",
    "\n",
    "p_wi_given_ham_dict = {}\n",
    "\n",
    "# P(w_i|Spam)\n",
    "for i in range(0, sms_spam_sum.size):\n",
    "    index = sms_spam_sum.index[i]\n",
    "    dividend = sms_spam_sum[index] + alpha\n",
    "    divisor = n_spam + (alpha*n_vocabulary)\n",
    "    p_wi_given_spam_dict[index] =  dividend / divisor\n",
    "\n",
    "    \n",
    "# P(w_i|Ham)\n",
    "for i in range(0, sms_ham_sum.size):\n",
    "    index = sms_ham_sum.index[i]\n",
    "    dividend = sms_ham_sum[index] + alpha\n",
    "    divisor = n_ham + (alpha*n_vocabulary)\n",
    "    p_wi_given_ham_dict[index] =  dividend / divisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking first 20 items in `p_wi_given_spam_dict` and `p_wi_given_ham_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cookies', 4.3036667240488894e-05),\n",
       " ('uncles', 4.3036667240488894e-05),\n",
       " ('shaping', 4.3036667240488894e-05),\n",
       " ('shoranur', 4.3036667240488894e-05),\n",
       " ('frank', 4.3036667240488894e-05),\n",
       " ('icmb3cktz8r7', 8.607333448097779e-05),\n",
       " ('150pm', 0.0003012566706834223),\n",
       " ('winning', 4.3036667240488894e-05),\n",
       " ('heater', 4.3036667240488894e-05),\n",
       " ('6669', 8.607333448097779e-05)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(p_wi_given_spam_dict.items())[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cookies', 4.621855212682371e-05),\n",
       " ('uncles', 7.703092021137284e-05),\n",
       " ('shaping', 3.081236808454914e-05),\n",
       " ('shoranur', 3.081236808454914e-05),\n",
       " ('frank', 3.081236808454914e-05),\n",
       " ('icmb3cktz8r7', 1.540618404227457e-05),\n",
       " ('150pm', 1.540618404227457e-05),\n",
       " ('winning', 3.081236808454914e-05),\n",
       " ('heater', 4.621855212682371e-05),\n",
       " ('6669', 1.540618404227457e-05)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(p_wi_given_ham_dict.items())[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free RAM 4\n",
    "\n",
    "del sms_spam\n",
    "del sms_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 8\n",
    "---\n",
    "Now that we've calculated all the constants and parameters we need, we can start creating the spam filter. The spam filter can be understood as a function that:\n",
    "\n",
    "- Takes in as input a new message ($w_1$, $w_2$, ..., $w_n$).\n",
    "\n",
    "\n",
    "- Calculates \n",
    ".\n",
    "  \n",
    "  \n",
    "- Compares the values of $P(Spam|w_1, w_2, ..., w_n)$ and $P(Ham|w_1, w_2, ..., w_n)$, and:\n",
    "    - If $P(Ham|w_1, w_2, ..., w_n) > P(Spam|w_1, w_2, ..., w_n)$, then the message is classified as ham.\n",
    "    \n",
    "    - If $P(Ham|w_1, w_2, ..., w_n) < P(Spam|w_1, w_2, ..., w_n)$, then the message is classified as spam.\n",
    "    \n",
    "    - If $P(Ham|w_1, w_2, ..., w_n) = P(Spam|w_1, w_2, ..., w_n)$, then the algorithm may request human help.\n",
    "\n",
    "Below, we see a rough sketch of how the spam filter function might look like:\n",
    "\n",
    "    import re\n",
    "\n",
    "    def classify(message):\n",
    "\n",
    "        message = re.sub('\\W', ' ', message)\n",
    "        message = message.lower()\n",
    "        message = message.split()\n",
    "\n",
    "        '''    \n",
    "        This is where we calculate:\n",
    "\n",
    "        p_spam_given_message = ?\n",
    "        p_ham_given_message = ?\n",
    "        '''    \n",
    "\n",
    "        print('P(Spam|message):', p_spam_given_message)\n",
    "        print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "        if p_ham_given_message > p_spam_given_message:\n",
    "            print('Label: Ham')\n",
    "        elif p_ham_given_message < p_spam_given_message:\n",
    "            print('Label: Spam')\n",
    "        else:\n",
    "            print('Equal proabilities, have a human classify this!')\n",
    "\n",
    "\n",
    "\n",
    "For the `classify()` function above, note that:\n",
    "\n",
    "\n",
    "- The input variable `message` is assumed to be a string.\n",
    "\n",
    "\n",
    "- We perform a bit of data cleaning on the string `message`:\n",
    "    - We remove the punctuation using the `re.sub()` function.\n",
    "    - We bring all letters to lower case using the `str.lower()` method.\n",
    "    - We split the string at the space character and transform it into a Python list using the `str.split()` method.\n",
    "    \n",
    "    \n",
    "- There's some placeholder code for calculating `p_spam_given_message` and `p_ham_given_message` — we'll write this code in the exercise below.\n",
    "\n",
    "\n",
    "- We compare `p_spam_given_message` with p_ham_given_message and then print a classification label.\n",
    "\n",
    "To write the code we need for calculating `p_spam_given_message` and `p_ham_given_message`, we need to use these two equations:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "Note that some new messages will contain words that are not part of the vocabulary. Recall from the previous lesson that we simply ignore these words when we're calculating the probabilities.\n",
    "\n",
    "Now we'll write the code for calculating `p_spam_given_message` and `p_ham_given_message`, and then we'll use the function to classify two new messages. On the next screen, we'll classify all the 1,114 messages in our test set.\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. Copy the `classify()` function you see above and write the code needed for calculating `p_spam_given_message` and `p_ham_given_message`.\n",
    "    - Initiate p_spam_given_message and p_ham_given_message with an initial value. We recommend initiating the variables as p_spam_given_message = p_spam and p_ham_given_message = p_ham (p_spam and p_ham are P(Spam) and P(Ham), and they were calculated on the previous steps).\n",
    "    - Iterate over each word in `message` (the input of the `classify()` function), which should be a list of strings by the time you start this loop. For each word:\n",
    "        - If the word is present in the dictionary containing the spam parameters, then update the value of `p_spam_given_message` by multiplying with the parameter value specific to that word. You'll need to code something similar to `p_spam_given_message *= parameters_spam[word]`.\n",
    "        - If the word is present in the dictionary containing the ham parameters, then update the value of `p_ham_given_message` by multiplying with the parameter value specific to that word. You'll need to do something like `p_ham_given_message *= parameters_spam[word]`.\n",
    "        - If the word is not present in any of the two dictionaries, then don't do anything. Recall that we ignore words that are not part of the vocabulary.\n",
    "\n",
    "\n",
    "\n",
    "3. Use the `classify()` function to classify two new messages. You can use any messages you want, but we suggest that one message is obviously spam, and the other is obviously ham. For instance, you can use these two messages:\n",
    "    - 'WINNER!! This is the secret code to unlock the money: C3421.'\n",
    "    - \"Sounds good, Tom, then see u there\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that $P(w_i|Spam)$ and $P(w_i|Ham)$ is calculated throughout the entire span of messages contained in the training set, it is possible to finally build the spam filter, by calculating and comparing $P(Spam|w_1, w_2, ..., w_n)$ with $P(Ham|w_1, w_2, ..., w_n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message):\n",
    "    \"\"\"Takes in a string - a cellphone message (SMS), and returns the probability of Spam given the input message,\n",
    "    the probability of non-spam (ham) given the input message and classifies whether\n",
    "    the message is spam, not spam (ham), or if a human is required to classify the message.\n",
    "    \"\"\"\n",
    "\n",
    "    message = re.sub('\\W', ' ', message) # still a string\n",
    "    message = message.lower() # still a string\n",
    "    message = message.split() # now a list of strings\n",
    "\n",
    "    \n",
    "    # Calculating P(Spam|w_1, w_2, ..., w_n) with P(Ham|w_1, w_2, ..., w_n).\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    # Note: if `word` is not in the spam or in the non-spam DataFrames the loop does nothing by default.\n",
    "    for word in message:\n",
    "        \n",
    "        if word in p_wi_given_spam_dict.keys():\n",
    "            p_spam_given_message *= p_wi_given_spam_dict[word]\n",
    "            \n",
    "        if word in p_wi_given_ham_dict.keys():\n",
    "            p_ham_given_message *= p_wi_given_ham_dict[word]\n",
    "        \n",
    "\n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "        \n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "        \n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify('WINNER!! This is the secret code to unlock the money: C3421.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify('Sounds good, Tom, then see u there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 2.075032030372528e-73\n",
      "P(Ham|message): 1.693000959967756e-66\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "classify(\"Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 9\n",
    "---\n",
    "On the previous screen, we managed to create a spam filter, and we classified two new messages. We'll now try to determine how well the spam filter does on our test set of 1,114 messages.\n",
    "\n",
    "The algorithm will output a classification label for every message in our test set, which we'll be able to compare with the actual label (given by a human). Note that, in training, our algorithm didn't see these 1,114 messages, so every message in the test set is practically new from the perspective of the algorithm.\n",
    "\n",
    "First off, we'll change the `classify()` function that we wrote previously to return the labels instead of printing them. Below, note that we now have `return` statements instead of `print()` functions:\n",
    "\n",
    "    def classify_test_set(message):\n",
    "\n",
    "        message = re.sub('\\W', ' ', message)\n",
    "        message = message.lower()\n",
    "        message = message.split()\n",
    "\n",
    "        p_spam_given_message = p_spam\n",
    "        p_ham_given_message = p_ham\n",
    "\n",
    "        for word in message:\n",
    "            if word in parameters_spam:\n",
    "                p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "            if word in parameters_ham:\n",
    "                p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "        if p_ham_given_message > p_spam_given_message:\n",
    "            return 'ham'\n",
    "        elif p_spam_given_message > p_ham_given_message:\n",
    "            return 'spam'\n",
    "        else:\n",
    "            return 'needs human classification'\n",
    "\n",
    "Now that we have a function that returns labels instead of printing them, we can use it to create a new column in our test set.\n",
    "\n",
    "    test_set['predicted'] = test_set['SMS'].apply(classify_test_set)\n",
    "    test_set.head()\n",
    "\n",
    "Output:\n",
    "\n",
    "|   |Label|SMS                                               |predicted|\n",
    "|---|-----|--------------------------------------------------|---------|\n",
    "|0  |ham  |Later i guess. I needa do mcat study too.         |ham      |\n",
    "|1  |ham  |But i haf enuff space got like 4 mb...            |ham      |\n",
    "|2  |spam |Had your mobile 10 mths? Update to latest Oran... |spam     |\n",
    "|3  |ham  |All sounds good. Fingers . Makes it difficult ... |ham      |\n",
    "|4  |ham  |All done, all handed in. Don't know if mega sh... |ham      |\n",
    "\n",
    "\n",
    "Now we can compare the predicted values with the actual values to measure how good our spam filter is with classifying new messages. To make the measurement, we'll use **accuracy** as a metric:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Accuracy} = \\frac{\\text{number of correctly classified messages}}{\\text{total number of classified messages}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "---\n",
    "1. Measure the accuracy of the spam filter.\n",
    "- Initialize a variable named correct with a value of `0`.\n",
    "- Initialize a variable named `total` with the number of messages in the test set.\n",
    "- Iterate over the test set DataFrame (you can use the `DataFrame.iterrows() method`). For each row:\n",
    "    - If the actual label is the same as the predicted label, then increment `correct` by `1`.\n",
    "    - Use `correct` and `total` in combination with the above formula to calculate the accuracy of the spam filter.\n",
    "\n",
    "2. What do you think about the accuracy value? Is it better or worse than you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of going for the suggested method in the instructions we'll add two new columns to `testing_set`: one, `Test`, displays the output returned by `classify_test_set()` for each row/message; the last, `Correct` is '1' if the filter returns the right classification, i.e. the same classification in the `Label` column (which previously classifies the message has 'spam' or 'ham'), or '0' otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_message = 'Sounds+good'\n",
    "\n",
    "\n",
    "# for index, value in enumerate(non_words_list) and enumerate(non_words_list_w_spaces):\n",
    "#     if non_words_list[index] in input_message:\n",
    "#         expression = non_words_list[index]\n",
    "#         expression = expression.encode('unicode_escape')\n",
    "#         input_message = re.sub(expression, non_words_list_w_spaces[index], input_message)\n",
    "\n",
    "# input_message "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "    \"\"\"Takes in a string - a cellphone message (SMS), and returns a classification of whether \n",
    "    the message is spam, not spam (ham), or if a human is required to classify the message.\n",
    "    \"\"\"\n",
    "\n",
    "    message = re.sub('[^A-Za-z0-9\\s£€\\$]', ' ', message) # still a string\n",
    "    message = re.sub('£', ' £ ', message)\n",
    "    message = re.sub('€', ' € ', message)\n",
    "    message = re.sub('\\$', ' $ ', message)\n",
    "    message = re.sub('\\s+', ' ', message)\n",
    "    message = message.lower() # still a string\n",
    "    message = message.split() # now a list of strings\n",
    "\n",
    "    \n",
    "    # Calculating P(Spam|w_1, w_2, ..., w_n) with P(Ham|w_1, w_2, ..., w_n).\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    # Note: if `word` is not in the spam or in the non-spam DataFrames the loop does nothing by default.\n",
    "    for word in message:\n",
    "        \n",
    "        if word in p_wi_given_spam_dict.keys():\n",
    "            p_spam_given_message *= p_wi_given_spam_dict[word]\n",
    "            \n",
    "        if word in p_wi_given_ham_dict.keys():\n",
    "            p_ham_given_message *= p_wi_given_ham_dict[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'Requires human classification.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set['Test'] = testing_set['SMS'].apply(classify_test_set)\n",
    "\n",
    "\n",
    "# Assigns True if condition is met and multplying by one converts True in '1' and False in '0'.\n",
    "testing_set['Correct'] = (testing_set['Label'] == testing_set['Test'])*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>Test</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4458</th>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "      <td>ham</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4459</th>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4460</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4462</th>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                                SMS  Test  Correct\n",
       "4458   ham          Later i guess. I needa do mcat study too.   ham        1\n",
       "4459   ham             But i haf enuff space got like 4 mb...   ham        1\n",
       "4460  spam  Had your mobile 10 mths? Update to latest Oran...  spam        1\n",
       "4461   ham  All sounds good. Fingers . Makes it difficult ...   ham        1\n",
       "4462   ham  All done, all handed in. Don't know if mega sh...   ham        1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of well classified messages when using the `classify_test_set()` function/filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When applied to the messages in the `training_set` (1114 entries) the test accuracy was aprox. 98.83%.\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = (testing_set['Correct'].sum() / testing_set.shape[0])*100\n",
    "\n",
    "test_accuracy = test_accuracy.round(2)\n",
    "\n",
    "print(f'When applied to the messages in the `testing_set` ({testing_set.shape[0]} entries) the test accuracy was aprox. {test_accuracy}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the value of the test accuracy of 98.74%, we can classify it as a success, since it exceeded the approval threshold of 80% by a substantial margin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slide 10 (end of project)\n",
    "---\n",
    "In this project, we managed to build a spam filter for SMS messages using the multinomial Naive Bayes algorithm. The filter had an accuracy of 98.74% on the test set, which is an excellent result. We initially aimed for an accuracy of over 80%, but we managed to do way better than that.\n",
    "\n",
    "If you want to keep working on this project, here's a few next steps you can take:\n",
    "\n",
    "\n",
    "1. Isolate the 14 messages that were classified incorrectly and try to figure out why the algorithm reached the wrong conclusions.\n",
    "\n",
    "\n",
    "2. Make the filtering process more complex by making the algorithm sensitive to letter case.\n",
    "\n",
    "\n",
    "3. Get the project portfolio-ready by using a few tips from our style guide for data science projects.\n",
    "\n",
    "\n",
    "Congratulations, this is the end of the Conditional Probability course! We've come a long way and learned how to:\n",
    "\n",
    "- Assign probabilities to events based on certain conditions by using conditional probability rules.\n",
    "\n",
    "\n",
    "- Assign probabilities to events based on whether they are in relationship of statistical independence or not with other events.\n",
    "\n",
    "\n",
    "- Assign probabilities to events based on prior knowledge by using Bayes' theorem.\n",
    "\n",
    "\n",
    "- Create a spam filter for SMS messages using the multinomial Naive Bayes algorithm.\n",
    "\n",
    "Curious to see what other students have done on this project? [Head over to our Community to check them out](https://community.dataquest.io/tags/c/social/share/49/433). While you are there, please remember to show some love and give your own feedback!\n",
    "\n",
    "And of course, we welcome you to share your own project and show off your hard work. Head over to our Community to [share your finished Guided Project](https://community.dataquest.io/tags/c/social/share/49/433)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Isolate the 14 messages that were classified incorrectly and try to figure out why the algorithm reached the wrong conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_incorrect = testing_set.Correct == 0\n",
    "\n",
    "incorrect = testing_set[cond_incorrect].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13 entries, 0 to 12\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Label    13 non-null     object\n",
      " 1   SMS      13 non-null     object\n",
      " 2   Test     13 non-null     object\n",
      " 3   Correct  13 non-null     int32 \n",
      "dtypes: int32(1), object(3)\n",
      "memory usage: 272.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "incorrect.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Not heard from U4 a while. Call me now am here all night with just my knickers on. Make me beg for it like U did last time 01223585236 XX Luv Nikiyu4.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Unlimited texts. Limited minutes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>26th OF JULY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nokia phone is lovly..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>A Boy loved a gal. He propsd bt she didnt mind. He gv lv lttrs, Bt her frnds threw thm. Again d boy decided 2 aproach d gal , dt time a truck was speeding towards d gal. Wn it was about 2 hit d girl,d boy ran like hell n saved her. She asked 'hw cn u run so fast?' D boy replied \"Boost is d secret of my energy\" n instantly d girl shouted \"our energy\" n Thy lived happily 2gthr drinking boost evrydy Moral of d story:- I hv free msgs:D;): gud ni8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>No calls..messages..missed calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spam</td>\n",
       "      <td>Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hi babe its Chloe, how r u? I was smashed on saturday night, it was great! How was your weekend? U been missing me? SP visionsms.com Text stop to stop 150p/text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>0A$NETWORKS allow companies to bill for SMS, so they are responsible for their \"suppliers\", just as a shop has to give a guarantee on what they sell. B. G.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spam</td>\n",
       "      <td>RCT' THNQ Adrian for U text. Rgds Vatian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>2/2 146tf150p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hello. We need some posh birds and chaps to user trial prods for champneys. Can i put you down? I need your address and dob asap. Ta r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  \\\n",
       "0   spam   \n",
       "1    ham   \n",
       "2    ham   \n",
       "3    ham   \n",
       "4    ham   \n",
       "5    ham   \n",
       "6    ham   \n",
       "7   spam   \n",
       "8   spam   \n",
       "9   spam   \n",
       "10  spam   \n",
       "11  spam   \n",
       "12  spam   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                               SMS  \n",
       "0                                                                                                                                                                                                                                                                                                        Not heard from U4 a while. Call me now am here all night with just my knickers on. Make me beg for it like U did last time 01223585236 XX Luv Nikiyu4.net  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                Unlimited texts. Limited minutes.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                     26th OF JULY  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                           Nokia phone is lovly..  \n",
       "4   A Boy loved a gal. He propsd bt she didnt mind. He gv lv lttrs, Bt her frnds threw thm. Again d boy decided 2 aproach d gal , dt time a truck was speeding towards d gal. Wn it was about 2 hit d girl,d boy ran like hell n saved her. She asked 'hw cn u run so fast?' D boy replied \"Boost is d secret of my energy\" n instantly d girl shouted \"our energy\" n Thy lived happily 2gthr drinking boost evrydy Moral of d story:- I hv free msgs:D;): gud ni8  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                 No calls..messages..missed calls  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us  \n",
       "7                                                                                                                                                                                                                                                                                                                                                       Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50  \n",
       "8                                                                                                                                                                                                                                                                                                 Hi babe its Chloe, how r u? I was smashed on saturday night, it was great! How was your weekend? U been missing me? SP visionsms.com Text stop to stop 150p/text  \n",
       "9                                                                                                                                                                                                                                                                                                      0A$NETWORKS allow companies to bill for SMS, so they are responsible for their \"suppliers\", just as a shop has to give a guarantee on what they sell. B. G.  \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                        RCT' THNQ Adrian for U text. Rgds Vatian  \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                   2/2 146tf150p  \n",
       "12                                                                                                                                                                                                                                                                                                                          Hello. We need some posh birds and chaps to user trial prods for champneys. Can i put you down? I need your address and dob asap. Ta r  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pd.options.display.max_colwidth = 500\n",
    "\n",
    "incorrect[['Label', 'SMS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'Oh my god! I ve found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_split = incorrect.SMS.str.replace('\\W', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                                                                                                                                                                                          Not heard from U4 a while  Call me now am here all night with just my knickers on  Make me beg for it like U did last time 01223585236 XX Luv Nikiyu4 net\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                  Unlimited texts  Limited minutes \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                       26th OF JULY\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                             Nokia phone is lovly  \n",
       "4     A Boy loved a gal  He propsd bt she didnt mind  He gv lv lttrs  Bt her frnds threw thm  Again d boy decided 2 aproach d gal   dt time a truck was speeding towards d gal  Wn it was about 2 hit d girl d boy ran like hell n saved her  She asked  hw cn u run so fast   D boy replied  Boost is d secret of my energy  n instantly d girl shouted  our energy  n Thy lived happily 2gthr drinking boost evrydy Moral of d story   I hv free msgs D    gud ni8\n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                   No calls  messages  missed calls\n",
       "6                                                                                                                                                                                                                                                                                                                                                                  We have sent JD for Customer Service cum Accounts Executive to ur mail id  For details contact us\n",
       "7                                                                                                                                                                                                                                                                                                                                                         Oh my god  I ve found your number again  I m so glad  text me back xafter this msgs cst std ntwk chg  1 50\n",
       "8                                                                                                                                                                                                                                                                                                   Hi babe its Chloe  how r u  I was smashed on saturday night  it was great  How was your weekend  U been missing me  SP visionsms com Text stop to stop 150p text\n",
       "9                                                                                                                                                                                                                                                                                                        0A NETWORKS allow companies to bill for SMS  so they are responsible for their  suppliers   just as a shop has to give a guarantee on what they sell  B  G \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                          RCT  THNQ Adrian for U text  Rgds Vatian\n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                     2 2 146tf150p\n",
       "12                                                                                                                                                                                                                                                                                                                            Hello  We need some posh birds and chaps to user trial prods for champneys  Can i put you down  I need your address and dob asap  Ta r\n",
       "Name: SMS, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_currency_1 = training_set.SMS.copy().apply(lambda x: re.match('$', x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FREE for 1st week! No1 Nokia tone 4 ur mob every week just txt NOKIA to 8007 Get txting and tell ur mates www.getzed.co.uk POBox 36504 W45WQ norm150p/tone 16+       3\n",
       "I don't know u and u don't know me. Send CHAT to 86688 now and let's find each other! Only 150p/Msg rcvd. HG/Suite342/2Lands/Row/W1J6HL LDN. 18 years or over.       3\n",
       "YES! The only place in town to meet exciting adult singles is now in the UK. Txt CHAT to 86688 now! 150p/Msg.                                                        2\n",
       "Congratulations ur awarded 500 of CD vouchers or 125gift guaranteed & Free entry 2 100 wkly draw txt MUSIC to 87066 TnCs www.Ldew.com1win150ppmx3age16               2\n",
       "Congrats! Nokia 3650 video camera phone is your Call 09066382422 Calls cost 150ppm Ave call 3mins vary from mobiles 16+ Close 300603 post BCM4284 Ldn WC1N3XX        2\n",
       "                                                                                                                                                                    ..\n",
       "YOU 07801543489 are guaranteed the latests Nokia Phone, a 40GB iPod MP3 player or a £500 prize! Txt word:COLLECT to No:83355! TC-LLC NY-USA 150p/Mt msgrcvd18+       1\n",
       "U can WIN £100 of Music Gift Vouchers every week starting NOW Txt the word DRAW to 87066 TsCs www.Idew.com SkillGame, 1Winaweek, age16. 150ppermessSubscription      1\n",
       "Dear U've been invited to XCHAT. This is our final attempt to contact u! Txt CHAT to 86688 150p/MsgrcvdHG/Suite342/2Lands/Row/W1J6HL LDN 18 yrs                      1\n",
       "Eerie Nokia tones 4u, rply TONE TITLE to 8007 eg TONE DRACULA to 8007 Titles: GHOST, ADDAMSFA, MUNSTERS, EXORCIST, TWILIGHT www.getzed.co.uk POBox36504W45WQ 150p    1\n",
       "Hi there, 2nights ur lucky night! Uve been invited 2 XCHAT, the Uks wildest chat! Txt CHAT to 86688 now! 150p/MsgrcvdHG/Suite342/2Lands/Row/W1J6HL LDN 18yrs         1\n",
       "Name: SMS, Length: 102, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containsd_gbp = training_set.SMS.str.contains('150p')\n",
    "\n",
    "find_currency_2 = training_set.SMS[containsd_gbp]\n",
    "\n",
    "find_currency_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: SMS, dtype: object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_currency_1[find_currency_1==True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123   maria 12caes    150 loly  loly  loly pussy net'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " re.sub('\\W', ' ', '123 @ maria 12caes £ £150 loly$ loly. loly+pussy.net')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Secret (?:!|,)  (?:!|,)  Money (?:!|,)  goods.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = '(?:!|,)'\n",
    "\n",
    "re.sub(pattern, f' {pattern} ', 'Secret!! Money, goods.' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3082\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 3",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-b1ff34d6e692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincorrect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#.str.replace('\\W', ' ', regex=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3081\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3083\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 3"
     ]
    }
   ],
   "source": [
    "one_line = incorrect[3]#.str.replace('\\W', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set_v2(message):\n",
    "    \"\"\"Takes in a string - a cellphone message (SMS), and returns a classification of whether \n",
    "    the message is spam, not spam (ham), or if a human is required to classify the message.\n",
    "    \"\"\"\n",
    "\n",
    "    message = re.sub('\\W', ' ', message) # still a string\n",
    "    message = message.lower() # still a string\n",
    "    message = message.split() # now a list of strings\n",
    "\n",
    "    \n",
    "    # Calculating P(Spam|w_1, w_2, ..., w_n) with P(Ham|w_1, w_2, ..., w_n).\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    # Note: if `word` is not in the spam or in the non-spam DataFrames the loop does nothing by default.\n",
    "    for word in message:\n",
    "        \n",
    "        if word in p_wi_given_spam_dict.keys():\n",
    "            p_spam_given_message *= p_wi_given_spam_dict[word]\n",
    "            \n",
    "        if word in p_wi_given_ham_dict.keys():\n",
    "            p_ham_given_message *= p_wi_given_ham_dict[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'Requires human classification.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "600.85px",
    "left": "1545px",
    "right": "20px",
    "top": "120px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
